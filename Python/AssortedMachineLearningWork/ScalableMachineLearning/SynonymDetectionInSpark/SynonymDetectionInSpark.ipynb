{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 - Synonym Detection In Spark\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "In the last homework assignment you performed Naive Bayes to classify documents as 'ham' or 'spam.' In doing so, we relied on the implicit assumption that the list of words in a document can tell us something about the nature of that document's content. We'll rely on a similar intuition this week: the idea that, if we analyze a large enough corpus of text, the list of words that appear in small window before or after a vocabulary term can tell us something about that term's meaning.\n",
    "\n",
    "This will be your first assignment working in Spark. You'll perform Synonym Detection by repurposing an algorithm commonly used in Natural Language Processing to perform document similarity analysis. In doing so you'll also become familiar with important datatypes for efficiently processing sparse vectors and a number of set similarity metrics (e.g. Cosine, Jaccard, Dice). By the end of this homework you should be able to:  \n",
    "* ... __define__ the terms `one-hot encoding`, `co-occurrance matrix`, `stripe`, `inverted index`, `postings`, and `basis vocabulary` in the context of both synonym detection and document similarity analysis.\n",
    "* ... __explain__ the reasoning behind using a word stripe to compare word meanings.\n",
    "* ... __identify__ what makes set-similarity calculations computationally challenging.\n",
    "* ... __implement__ stateless algorithms in Spark to build stripes, inverted index and compute similarity metrics.\n",
    "* ... __apply__ appropriate metrics to assess the performance of your synonym detection algorithm. \n",
    "\n",
    "\n",
    "__`NOTE`__: your reading assignment for weeks 5 and 6 were fairly heavy and you may have glossed over the papers on dimension independent similarity metrics by [Zadeh et al](http://stanford.edu/~rezab/papers/disco.pdf) and pairwise document similarity by [Elsayed et al](https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf). If you haven't already, this would be a good time to review those readings -- they are directly relevant to this assignment.\n",
    "\n",
    "__Please refer to the `README` for homework submission instructions and additional resources.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session (RUN THIS CELL AS IS)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'hw3_notebook'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.port', '42763'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'docker.w261'),\n",
       " ('spark.app.id', 'local-1560395862774')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark configuration Information (RUN THIS CELL AS IS)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Spark Basics.\n",
    "In your readings and live session demos for weeks 4 and 5 you got a crash course in working with Spark. We also talked about how Spark RDDs fit into the broader picture of distributed algorithm design. The questions below cover key points from these discussions. Feel free to answer each one very briefly.\n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What is Spark? How  does it relate to Hadoop MapReduce?\n",
    "\n",
    "* __b) short response:__ In what ways does Spark follow the principles of statelessness (a.k.a. functional programming)? List at least one way in which it allows the programmer to depart from this principle. \n",
    "\n",
    "* __c) short response:__ In the context of Spark what is a 'DAG' and how do they relate to the difference between an 'action' and a 'transformation'? Why is it useful to pay attention to the DAG that underlies your Spark implementation?\n",
    "\n",
    "\n",
    "* __d) short response:__ Give a specific example of when we would want to `cache()` an RDD and explain why.\n",
    "\n",
    "* __e) environment:__ List your environment configuration in which you will be working for this assignment, including OS, Number of Cores, CPUs, Memory. Are you using Docker? Are you running in the cloud (what is your cloud Spark configuration)? (If you are working on a Mac, you can use the these commands to get the system configuration: `sw_vers` and `system_profiler SPHardwareDataType`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Student Answers:\n",
    "> __a)__ Spark is a distributed computing system that allowes for high performance large scale data processing.  Like Hadoop MapReduce, it can parallelize processing tasks by partitioning the data and executing computation on different nodes.  Unlike MapReduce, which must write results to disk after each job to be accessed by subsequent MapReduce jobs, Spark has the ability to cache results for later downstream operations.  Spark also unique in its 'lazy' evaluation, which means that transformation functions do not perform any actual work until an action is called. \n",
    "\n",
    "> __b)__ Spark follows the principles of statelessness in the sense that the implementations can be built so they do not depend on mutable information, or changing state of data structures.  An implementation that consists of only RDDs, for instance, would follow the principles of functional programming because none of the RDDs are mutable.  Each function takes a previous RDD as an imput and produces a new RDD as an output. One way that Spark can deviate from statelessness, however, is in the use of an accumulator. Accumulators are write only variables which can change state, as in their value, over the course of a job. \n",
    "\n",
    "> __c)__ A DAG is a Directed Acyclic Graph, which is a representation of the list of transformations needed to create each RDD in Spark job.  They are essential to the concept of lazy evaluation.  The list of transformations do not actually create any RDDs if they are run alone.  It is not until an action is called that the list of transformations will produce the RDDs.  It is important to pay attention to DAGs in a spark implementation for both debugging and complexity considerations. For instance, a Spark job that takes too long to execute may be due to inefficiently created data structures at each stage.  Looking at how a DAG changes with new achitectural decisions is a way to gauge complexity and improve the performance of a Spark job. \n",
    "\n",
    "> __d)__  Cache()-ing allows us to use the contents of an RDD from one or a series of operation in later downstream operations. Due to lazy evaluation, actions downstream will run all previous transformations in the DAG from the beginning if there is no cache()-ing at any stage.  An example of when we would want to cache is if we are planning to reuse the output from a complex series of transformations that has already taken a very long time to run.  For instance, in the Naive Bayes execution in Spark, it makes sense to cache the word count outputs from training the data during the construction of the model, since this result is used to build the model probilitites for each word and the priors.  Since this RDD of word counts takes a long time to construct when passing through a relatively large corpus, it would make sense to cache() the results from the training set for later computations, so that in case of error or debugging, the DAG would not need to run from scratch.\n",
    "\n",
    "> __e)__ The operating system is Mac OS X, version 10.14.3. There are 2 cores, 14 GB of Memory and 4 CPUs.  I am using Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Similarity Metrics\n",
    "As mentioned in the introduction to this assignment, an intuitive way to compare the meaning of two documents is to compare the list of words they contain. Given a vocabulary $V$ (feature set) we would represent each document as a vector of `1`-s and `0`-s based on whether or not it contains each word in $V$. These \"one-hot encoded\" vector representations allow us to use math to identify similar documents. However like many NLP tasks the high-dimensionality of the feature space is a challenge... especially when we start to scale up the size and number of documents we want to compare.\n",
    "\n",
    "In this question we'll look at a toy example of document similarity analysis. Consider these 3 'documents': \n",
    "```\n",
    "docA\tthe flight of a bumblebee\n",
    "docB\tthe length of a flight\n",
    "docC\tbuzzing bumblebee flight\n",
    "```\n",
    "These documents have a total of $7$ unique words: \n",
    ">`a, bumblebee, buzzing, flight, length, of, the`.     \n",
    "\n",
    "Given this vocabulary, the documents' vector representations are (note that one-hot encoded entries follow the order of the vocab list above):\n",
    "\n",
    "```\n",
    "docA\t[1,1,0,1,0,1,1]\n",
    "docB\t[1,0,0,1,1,1,1]\n",
    "docC\t[0,1,1,1,0,0,0]\n",
    "```  \n",
    "\n",
    "### Q2 Tasks:\n",
    "\n",
    "* __a) short response:__ The cosine similarity between two vectors is $\\frac{A\\cdot B}{|A||B|}$. Explain what the the numerator and denominator of this calculation would represent in terms of word counts in documents A and B. \n",
    "\n",
    "* __b) short response:__ Explain how the Jaccard, Overlap and Dice metrics are similar/different to the calculation for cosine similarity. When would these metrics lead to different similarity rankings for a set of documents?\n",
    "\n",
    "* __c) short response:__ Calculate the cosine similarity for each pair of documents in our toy corpus. Please use markdown and $\\LaTeX$ to show your calcuations.  \n",
    "\n",
    "* __d) short response:__ According to your calculations in `part c` which pair of documents are most similar in meaning? Does this match your expecatation from reading the documents? If not, speculate about why we might have gotten this result.\n",
    "\n",
    "* __e) short response:__ In NLP common words like '`the`', '`of`', and '`a`' increase our feature space without adding a lot of signal about _semantic meaning_. Repeat your analysis from `part c` but this time ignore these three words in your calculations [__`TIP:`__ _to 'remove' stopwords just ignore the vector entries in columns corresponding to the words you wish to disregard_]. How do your results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Student Answers:\n",
    "> __a)__ In cosine similarity, the numerator is the dot product of the vector representations of docs A and B, which computes the overlap between the two documents, or the total number of unique words they share.  The denominator is a product of the lengths of the documents used to normalize this measurement.  However, in cosine similarity, the document lengths represented by |A| and |B| in the denominator are not the counts of the number of words, but can be used as a proxy for the count of the number of words, as they are the square root of the number of words in each document.  These lengths are the Euclidean lengths of both vectors in n-dimensional space, for n unique words. \n",
    "\n",
    "> __b)__ Jaccard, Overlap and Dice metrics all have the same numerator, or the count of the number of unique words shared, as the Cosine similarity metric, but they have different denominators.  In the Jaccard metric, the denominator is length of the union of two documents, or the count of the total number of unique words that are in either document A or B.  This is different from Cosine similarity in that the Jaccard denominator also takes into consideration the shared words between A and B whereas Cosine similarity demominator computes the document lengths independently of each other and does not take the shared words into consideration. In the Overlap metric, the denominator is only the number of unique words in the shorter vector between A and B, and completely disregards the length of the document with more words.  The Dice metric is yet again different, in that the denominator is the average of the number of unique words in documents A and B.  (please note, that I interpret the Dice metric using this format $\\dfrac{\\#(x,y)}{\\dfrac{\\#x+\\#y}{2}}$ which is equivalent to Zadeh et al paper notation of $\\dfrac{2\\#(x,y)}{\\#x+\\#y}$, so that I can just compare only denominators across the 4 metrics where the numerator is the same).\n",
    "\n",
    "> To consider how the similarity rankings might differ, let's first consider an instance in which the number of words are different in document A than in document B.  Suppose document B has the greater number of words.  In this example, the Overlap metric would be the largest of the 4, since it uses only the length of document A as a denominator.  We know right away that all 3 metrics are greater for the following reasons.  Dice metric denominator takes the average of the 2 document lengths, which must be the midpoint between the length of A and length of B, which is greater than A, therefore Dice < Overlap due to a larger denominator.  Cosine similarity is also smaller because denominator |A||B| > |A||A| given that B is longer, so Cosine < Overlap due to larger denoominator.  Jaccard's denominator takes the length of the union of the two documents, which must be greater than the length of A.  In the best case scenario, if all of the words in document A is contained in document B, Jaccard metric's denominator is at least the length of document B, which is greater than document A.  Therefore, we have the Jaccard < Overlap as well, due to the larger denominator.  Next, we see that the denominator of Jaccard, which is atleast the length of B, must be greater than the both Dice (less than length of B) and Cosine (less than length of B, given that it is $\\sqrt{AB}$).  Therefore, Jaccard metric would be the least given that it has the greatest denominator.  Lastly, we make a comparison between Cosine and Dice, and by plugging in a few numbers, we can see that the denominator of Cosine is always smaller than the denominator of Dice for different number of words in A and B.  So for documents with different number of words, we see that rankings of the similarity metrics as Jaccard < Dice < Cosine < Overlap.\n",
    "\n",
    "> For documents A and B that have the same number of words, Dice, Cosine and Overlap metrics would have the same denominator.  Jaccard's denominator is only equivalent to the other 3, if A and B have the same exact words, otherwise it is greater. Therefore, for the same number of words in two documents, we have that Jaccard < Dice = Cosine = Overlap, unless the documents are identical, in which case all 4 metrics are the same.  \n",
    "\n",
    "\n",
    "> __c)__ Caculations below:\n",
    "\n",
    "$cos(A,B)=\\dfrac{1\\cdot1+1\\cdot0+0\\cdot0+1\\cdot1+0\\cdot1+1\\cdot1+1\\cdot1}{\\sqrt{5*1^{2}}\\cdot\\sqrt{5*1^{2}}}=\\dfrac{4}{5}=0.8$\n",
    "\n",
    "$cos(B,C)=\\dfrac{1\\cdot0+0\\cdot1+0\\cdot1+1\\cdot1+1\\cdot0+1\\cdot0+1\\cdot0}{\\sqrt{5*1^{2}}\\cdot\\sqrt{3*1^{2}}}=\\dfrac{1}{\\sqrt{15}}\\approx0.26$\n",
    "\n",
    "$cos(A,C)=\\dfrac{1\\cdot0+1\\cdot1+0\\cdot1+1\\cdot1+0\\cdot0+1\\cdot0+1\\cdot0}{\\sqrt{5*1^{2}}\\cdot\\sqrt{3*1^{2}}}=\\dfrac{2}{\\sqrt{15}}\\approx0.516$\n",
    "\n",
    "> __d)__ According to the cosine similarity calculations, documents A and B have the most similarity.  However, this does not meet my expectations as when I read the documents, I feel that A and C are the most similar since they both refer to bumblebee flying whereas A does not.\n",
    "\n",
    "> __e)__ If we ignore the stop words, our unique words vector space only has 4 words: [bumblebee, buzzing, flight, length].  Our vector representations then become:\n",
    "\n",
    "doc A: [1,0,1,0]\n",
    "\n",
    "doc B: [0,0,1,1]\n",
    "\n",
    "doc C: [1,1,1,0]\n",
    "\n",
    "Calculations for Cosine Similarity are:\n",
    "\n",
    "$cos(A,B)=\\dfrac{1\\cdot0+0\\cdot0+1\\cdot1+0\\cdot1}{\\sqrt{2*1^{2}}\\cdot\\sqrt{2*1^{2}}}=\\dfrac{1}{2}=0.5$\n",
    "\n",
    "$cos(B,C)=\\dfrac{0\\cdot1+0\\cdot1+1\\cdot1+1\\cdot0}{\\sqrt{2*1^{2}}\\cdot\\sqrt{3*1^{2}}}=\\dfrac{1}{\\sqrt{6}}\\approx0.41$\n",
    "\n",
    "$cos(A,C)=\\dfrac{1\\cdot1+0\\cdot1+1\\cdot1+0\\cdot0}{\\sqrt{2*1^{2}}\\cdot\\sqrt{3*1^{2}}}=\\dfrac{2}{\\sqrt{6}}\\approx0.816$\n",
    "\n",
    "In this analysis where we have excluded stop words, we see that the cosine similarity metric is highest for documents A and C.  This matches my expectations much more as A and C are closest in meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Synonym Detection Strategy\n",
    "\n",
    "In the Synonym Detection task we want to compare the meaning of words, not documents. For clarity, lets call the words whose meaning we want to compare `terms`. If only we had a 'meaning document' for each `term` then we could easily use the document similarity strategy from Question 2 to figure out which `terms` have similar meaning (i.e. are 'synonyms'). Of course in order for that to work we'd have to reasonably believe that the words in these 'meaning documents' really do reflect the meaning of the `term`. For a good analysis we'd also need these 'meaning documents' to be fairly long -- the one or two sentence dictionary definition of a term isn't going to provide enough signal to distinguish between thousands and thousands of `term` meanings.\n",
    "\n",
    "This is where the idea of co-occurrance comes in. Just like DocSim makes the assumption that words in a document tell us about the document's meaning, we're going to assume that the set of words that 'co-occur' within a small window around our term can tell us some thing about the meaning of that `term`. Remember that we're going to make this 'co-words' list (a.k.a. 'stripe') by looking at a large body of text. This stripe is our 'meaning document' in that it reflects all the kinds of situations in which our `term` gets used in real language. So another way to phrase our assumption is: we think `terms` that get used to complete lots of the same phrases probably have related meanings. This may seem like an odd assumption but computational linguists have found that it works surprisingly well in practice. Let's look at a toy example to build your intuition for why and how.\n",
    "\n",
    "Consider the opening line of Charles Dickens' _A Tale of Two Cities_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom it was the age of foolishness\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 10 unique words in this short 'corpus':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'times', 'wisdom', 'worst', 'was', 'best', 'foolishness', 'it', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "words = list(set(re.findall(\"\\w+\", corpus.lower())))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of these 10 words, 4 are so common that they probably don't tell us very much about meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"it\", \"the\", \"was\", \"of\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll ignore these 'stop words' and we're left with a 6 word vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted([w for w in words if w not in stopwords])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in the tasks below is to asses, which of these six words are most related to each other in meaning -- based solely on this short two line body of text.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ Given this six word vocabulary, how many 'pairs' of words do we want to compare? More generally for a n-word vocabulary how many pairwise comparisons are there to make? \n",
    "\n",
    "* __b) code:__ In the space provided below, create a 'stripe' for each `term` in the vocabulary. This stripe should be the list of all other vocabulary words that occur within a __5 word window__ (two words on either side) of the `term`'s position in the original text.\n",
    "\n",
    "* __c) code + short response:__ Complete the provided code to turn your stripes into a 1-hot encoded co-occurrence matrix. For our 6 word vocabulary how many entries are in this matrix? How many entries are zeros? \n",
    "\n",
    "* __d) code:__ Complete the provided code to loop over all pairs and compute their cosine similarity. Please do not modify the existing code, just add your own in the spot marked.\n",
    "\n",
    "* __e) short response:__ Which pairs of words have the highest 'similarity' scores? Are these words 'synonyms' in the traditional sense? In what sense are their meanings 'similar'? Explain how our results are contingent on the input text. What would change if we had a much larger corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__ For n-word vocabulary, the number of pairwise comparisons we need to make is (n C 2) or (n choose 2), since the order of the pairs does not matter and we should not count (times,age) and (age, times) as different pairs for instances.  (n C 2) = $\\dfrac{n!}{(n-2)!(2!)}=\\dfrac{n*(n-1)}{2}$.  For n=6, the total number of comparison is then $\\dfrac{6\\cdot5}{2}=15$\n",
    "\n",
    "> __c)__ There are 6x6 or 36 entries in teh 1-hot encoded co-occurrence matrix. The matrix has 8 entries that are 1's so there are 36-8 = 28 entries that are 0s.  \n",
    "\n",
    "> __e)__ The pairs of words withthe highest similarity scores are (best, worst) and (foolishness, wisdom). They are actually not synonyms in that their meanings are not similar.  In fact both pairs of words are examples of antonyms, because their meanings are opposite each other.  These results occurred because the two words in each pair have the exact same vocab words in their 5 word window, leading identical stripes.  It just so happens that the corpus we used had a lot of symmetry in each line around the two words in each pair, perhaps added for dramatic effect.  If we have a much larger corpus, we can expect additional appearances of all of the words in the vocab, leading to longer stripes and it is then possible that these pairs will not have as high cosine similarity values as other pairs, for instance the pair (age, times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS:\n",
      "It was the best of times, it was the worst of times, \n",
      "it was the age of wisdom it was the age of foolishness\n",
      "VOCAB:\n",
      "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "# for convenience, here are the corpus & vocab list again (RUN THIS CELL AS IS)\n",
    "print(\"CORPUS:\")\n",
    "print(corpus)\n",
    "print('VOCAB:')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename=\"best-of-times.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - USE THE TEXT ABOVE TO COMPLETE EACH STRIPE\n",
    "stripes = {'age':['wisdom','foolishness'], # example\n",
    "           'best':['times'], # YOU FILL IN THE REST\n",
    "           'foolishness':['age'],\n",
    "           'times': ['best', 'worst'],\n",
    "           'wisdom':['age'],\n",
    "           'worst':['times']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - initializing an empty co-occurrence matrix (RUN THIS CELL AS IS)\n",
    "co_matrix = pd.DataFrame({term: [0]*len(vocab) for term in vocab}, index = vocab, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>times</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foolishness</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisdom</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  best  foolishness  times  wisdom  worst\n",
       "age            0     0            1      0       1      0\n",
       "best           0     0            0      1       0      0\n",
       "foolishness    1     0            0      0       0      0\n",
       "times          0     1            0      0       0      1\n",
       "wisdom         1     0            0      0       0      0\n",
       "worst          0     0            0      1       0      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part c - FILL IN THE MISSING LINE so that this cell 1-hot encodes the co-occurrence matrix\n",
    "for term, nbrs in stripes.items():\n",
    "    for nbr in nbrs:\n",
    "        #pass\n",
    "        ############# YOUR CODE HERE #################\n",
    "        if nbr in vocab:\n",
    "            co_matrix[term][vocab.index(nbr)] = 1\n",
    "        \n",
    "        ############# (END) YOUR CODE #################\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age-best: 0.0\n",
      "age-foolishness: 0.0\n",
      "age-times: 0.0\n",
      "age-wisdom: 0.0\n",
      "age-worst: 0.0\n",
      "best-foolishness: 0.0\n",
      "best-times: 0.0\n",
      "best-wisdom: 0.0\n",
      "best-worst: 1.0\n",
      "foolishness-times: 0.0\n",
      "foolishness-wisdom: 1.0\n",
      "foolishness-worst: 0.0\n",
      "times-wisdom: 0.0\n",
      "times-worst: 0.0\n",
      "wisdom-worst: 0.0\n"
     ]
    }
   ],
   "source": [
    "# part d - FILL IN THE MISSING LINES to compute the cosine similarity between each pair of terms\n",
    "for term1, term2 in itertools.combinations(vocab, 2):\n",
    "    # one hot-encoded vectors\n",
    "    v1 = co_matrix[term1]\n",
    "    v2 = co_matrix[term2]\n",
    "    \n",
    "    # cosine similarity\n",
    "    ############# YOUR CODE HERE #################\n",
    "    #initialize as 1-D arrays to use numpy functions np.dot() and np.sum()\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    \n",
    "    #implement cosine sim equation\n",
    "    csim = np.dot(v1, v2)/(np.sum(np.dot(v1, v1))*np.sum(np.dot(v2,v2)))**0.5\n",
    "    ############# (END) YOUR CODE #################    \n",
    "    \n",
    "    print(f\"{term1}-{term2}: {csim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Pairs and Stripes at Scale\n",
    "\n",
    "As you read in the paper by Zadeh et al, the advantage of metrics like Cosine, Dice, Overlap and Jaccard is that they are dimension independent -- that is to say, if we implement them in a smart way the computational complexity of performing these computations is independent of the number of documents we want to compare (or in our case, the number of terms that are potential synonyms). One component of a 'smart implementation' involves thinking carefully both about how you define the \"basis vocabulary\" that forms your feature set (removing stopwords, etc). Another key idea is to use a data structure that facilitates distributed calculations. The DISCO implemetation further uses a sampling strategy, but that is beyond the scope of this assignment. \n",
    "\n",
    "In this question we'll take a closer look at the computational complexity of the synonym detection approach we took in question 3 and then revist the document similarity example as a way to explore a more efficient approach to parallelizing this analysis.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) short response:__ In question 3 you calculated the cosine similarity of pairs of words using the vector representation of their co-occurrences in a corpus. Imagine for now that you have unlimited memory on each of your nodes and describe a sequence of map & reduce steps that would start from a raw corpus and reproduce your strategy from Q3. For each step be sure to note what information would be stored in memory on your nodes and what information would need to be shuffled over the network (a bulleted list of steps with 1-2 sentences each is sufficient to answer this question).\n",
    "\n",
    "* __b) short response:__ In the asynch videos about \"Pairs and Stripes\" you were introduced to an alternative strategy. Explain two ways that using these data structures are more efficient than 1-hot encoded vectors when it comes to distributed similarity calculations [__`HINT:`__ _Consider memory constraints, amount of information being shuffled, amount of information being transfered over the network, and level of parallelization._]\n",
    "\n",
    "* __c) read provided code:__ The code below provides a streamined implementation of Document similarity analysis in Spark. Read through this code carefully. Once you are confident you understand how it works, answer the remaining questions. [__`TIP:`__ _to see the output of each transformation try commenting out the subsequent lines and adding an early `collect()` action_.]\n",
    "\n",
    "* __d) short response:__ The second mapper function, `splitWords`, emits 'postings'. The list of all 'postings' for a word is also refered to as an 'inverted index'. Define each of these terms based on your reading of the provided code.\n",
    "\n",
    "* __e) short response:__ The third mapper, `makeCompositeKeys`, loops over the inverted index to emit 'pairs' of what? Explain what information is included in the composite key created at this stage and why it makes sense to synchronize around that information in the context of performing document similarity calculations. In addition to the information included in these new keys, what other piece of information will we need to compute Jaccard or Cosine similarity?\n",
    "\n",
    "* __f) short response:__ Out of all the Spark transformations we make in this analysis, which are 'wide' transformations and which are 'narrow' transformations. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a)__ The following list is the series of maps and reduces that would have produced the strategy for Q3, given unlimited memory:\n",
    " - In the first mapper, we scan through each 5 consecutive word window in the corpus and creates pairs of all of the words that are not included in the list of stop words.  For instance, if all 5 words are not stop words, we should have 5 C 2 = 10 pairs.  In order to get all the pairs, we would need to commit each window of words to memory temporarily.  The mapper then emits each word pair with a count of 1 and also each reverse of the pair with a count of 1.  The reason that each pair and reverse pair is counted is because I would try to recreate the one-hot encoded matrix entries from Q3, which means that there is an entry for the words (A, B) and a separate entry for the words (B,A).  The keys from this mapper are the pairs and the values are the counts of 1\n",
    " - In the first reducer step, I would then sum all of the values for the same identical pair key.  The keys and their sums will be emitted from this reducer.  The reducers should essentially emit all of the entries of the one-hot encoded matrix we used in Q3. \n",
    " - In a second mapper, I will transform each key-value pair from the first reducer.  The first word in the word pair key will become the new key. The value will then be a dictionary entry of the second word and the count of total occurrences of he pair.  So for each \"(first word, second word)\\tcount\" row, the second mapper emits \"first word \\t{second word: count}\"\n",
    " - the second reducer combines all key value pairs from the second mapper, such that all values with the same first word key will be combined into a larger dictionary.  The result that is emitted by the second reducer should be of the format \"first word\\t {second word 1: count 1, second word 2: count 2. second word 3: count 3, .....}\".  Essentially the output from the second reducer should now have each keys that are the index values of the one-hot encoded matrix, and the values are dictionaries represent the rows, where each dictionary key is a column header and each dictionary value is the actual matrix entry.  Based on the previous steps, only non-zero values are included in the dictionary entries that re values. \n",
    " - In a third mapper, I would take the outputs from the second reducer and append a total count to the values.  The output format would then be \"first word\\t[{second word 1: count 1, second word 2: count 2. second word 3: count 3, .....}, total count]\", where total count is the number of dictionary entries or the set of unique second words that have co-corrence with the first word.  \n",
    " - At this point, we have all of the information we need to do the calculations for cosine similarity.  However, we will not be able to make all pair wise calculations, by pairs of the first word key, if the keys are passed to different nodes. Therefore, we would need to collect() all of our results from the previous steps into one RDD and cache() this result.\n",
    " - In a 4th mapper, we will need to create all possible pairs of the keys that were collected in the previous step. For n total keys, or n unique words in the corpus, we would have n(n-2)/2 pairs.  In this mapper, since we have unlimited memory on each node, we could output key-value pairs of the following format \"(first word A, first word B)\\t[dictionary A, dictionary B, total count A, total count B]\", although this is not particularly efficient.\n",
    " - Then in the 4th reducer, we would perform all pairwise cosine similarity calculations.  Using the two dictionaries, we would sum all values pertaining to dictionary keys that appear in both dictionary A and dictionary B.  We then divide by the sum by the sqrt of total count A and total count B.  The 4th reducer would output these pairs and their cosine similarit results.\n",
    "\n",
    "> __b)__ If we initially create stripes in a mapper, rather than pairs, we would have fewer key-value pairs to communicate between mapper and reducer.  This will greatly decrease the amount of information that is shuffled and transfered over network because instead of shuffling numerous individual pairs and their counts, we are shuffling words and some of their co-occuring words in the form of a stripe.  In addition, we do not need to double count pairs (A, B) and (B, A) for one hot encoded entries, so we reduce the number of key-value pairs that are shuffled and transfered by several times.  Furthermore, we are reducing the number of mapper and reducer steps that are needed.  Basically in one mapper and reducer step, we are able to get every unique word as a key and the unique list of all co-occuring words for each word, and the counts of co-occurence.  This also reduces the time and amount of effort to get to get to the results that then allow the subsquently computation the cosine similarities. \n",
    "\n",
    "> __c)__ _read provided code before answering d-f_ \n",
    "\n",
    "> __d)__ In the second mapper using splitWords(), the emitted key is a word in each document and emitted value is a tuple where the first tuple value is the document that the word appears in and the second tuple value is the number of words in that document.  The \"posting\" is esentially the tuple of the document ID and the word count within that document, for the document associated with the word key.  The word is an \"inverted index\" because this mapper essentially applies an inversion to its input.  The inputs to the mapper use the document ID as the key, and the words as the value.  However, the output of the mapper use the individual words as the key and the document IDs are now incorporated into the values.  The splitWords() mapper essentially performs a matrix inversion in which the row index from the input matrix has become the values in the output matrix and the values from the input matrix has become the row indexes in the output matrix.\n",
    "\n",
    "> __e)__ The third mapper, makeCompositeKey(), makes pairs of all of the values within the list of postings that are associated with each word.  At this point, when we use this mapper, we have already performed a reducing step where we have each unique word in all documents as the key and the values are all of the tuples where the first value is the document that the word appears in and the second value is the size of the document.  In this mapper, we create all possible pairs of postings, and form a composite key of two tuples, then emit the value of 1 for each composite key.  It makes sense to synchronize around this information, which is all possible pairs of documents, because this is the only way to perform similarity metrics between documents.  If we did not create composite keys in this way, we would have needed to commit all of the documents to memory in order to make all possible pairwise calculations, which is very costly.  This transformation allows us to only count document pairs that truly share 1 or more words. In order to compute a metric like Jaccard or Cosine similarity, we are still missing the total dot product, or the total number of unique words that appear in each document pair.  That is why there is an additional reduce step to get this total.\n",
    "\n",
    "> __f)__ The narrow spark transformations are the inputs to the 3 flatMap() functions which can take place within the individual partitions.  The wide spark transformations are the 2 reduceByKey() functions which require shuffling and aggregation of results from different nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test file: __`sample_docs.txt`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_docs.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_docs.txt\n",
    "docA\tbright blue butterfly forget\n",
    "docB\tbest forget bright sky\n",
    "docC\tblue sky bright sun\n",
    "docD\tunder butterfly sky hangs\n",
    "docE\tforget blue butterfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Document Similarity Analysis in Spark:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data - RUN THIS CELL AS IS\n",
    "data = sc.textFile(\"sample_docs.txt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def splitWords(pair):\n",
    "    \"\"\"Mapper 2: tokenize each document and emit postings.\"\"\"\n",
    "    doc, text = pair\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        yield (w, [(doc,len(words))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def makeCompositeKey(inverted_index):\n",
    "    \"\"\"Mapper 3: loop over postings and yield pairs.\"\"\"\n",
    "    word, postings = inverted_index\n",
    "    # taking advantage of symmetry, output only (a,b), but not (b,a)\n",
    "    for subset in itertools.combinations(sorted(postings), 2):\n",
    "        yield (str(subset), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def jaccard(line):\n",
    "    \"\"\"Mapper 4: compute similarity scores\"\"\"\n",
    "    (doc1, n1), (doc2, n2) = ast.literal_eval(line[0])\n",
    "    total = int(line[1])\n",
    "    jaccard = total / float(int(n1) + int(n2) - total)\n",
    "    yield doc1+\" - \"+doc2, jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*\\*\\*Following is my scratch work to help with interpretation of the Spark Job\\*\\*\\***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['docA', 'bright blue butterfly forget'],\n",
       " ['docB', 'best forget bright sky'],\n",
       " ['docC', 'blue sky bright sun'],\n",
       " ['docD', 'under butterfly sky hangs'],\n",
       " ['docE', 'forget blue butterfly']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda line: line.split('\\t')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bright', [('docA', 4)]),\n",
       " ('blue', [('docA', 4)]),\n",
       " ('butterfly', [('docA', 4)]),\n",
       " ('forget', [('docA', 4)]),\n",
       " ('best', [('docB', 4)]),\n",
       " ('forget', [('docB', 4)]),\n",
       " ('bright', [('docB', 4)]),\n",
       " ('sky', [('docB', 4)]),\n",
       " ('blue', [('docC', 4)]),\n",
       " ('sky', [('docC', 4)]),\n",
       " ('bright', [('docC', 4)]),\n",
       " ('sun', [('docC', 4)]),\n",
       " ('under', [('docD', 4)]),\n",
       " ('butterfly', [('docD', 4)]),\n",
       " ('sky', [('docD', 4)]),\n",
       " ('hangs', [('docD', 4)]),\n",
       " ('forget', [('docE', 3)]),\n",
       " ('blue', [('docE', 3)]),\n",
       " ('butterfly', [('docE', 3)])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda line: line.split('\\t')).flatMap(splitWords).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bright', [('docA', 4), ('docB', 4), ('docC', 4)]),\n",
       " ('butterfly', [('docA', 4), ('docD', 4), ('docE', 3)]),\n",
       " ('best', [('docB', 4)]),\n",
       " ('sky', [('docB', 4), ('docC', 4), ('docD', 4)]),\n",
       " ('sun', [('docC', 4)]),\n",
       " ('blue', [('docA', 4), ('docC', 4), ('docE', 3)]),\n",
       " ('forget', [('docA', 4), ('docB', 4), ('docE', 3)]),\n",
       " ('under', [('docD', 4)]),\n",
       " ('hangs', [('docD', 4)])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda line: line.split('\\t')).flatMap(splitWords).reduceByKey(lambda x,y : x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"(('docA', 4), ('docB', 4))\", 1),\n",
       " (\"(('docA', 4), ('docC', 4))\", 1),\n",
       " (\"(('docB', 4), ('docC', 4))\", 1),\n",
       " (\"(('docA', 4), ('docD', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 1),\n",
       " (\"(('docD', 4), ('docE', 3))\", 1),\n",
       " (\"(('docB', 4), ('docC', 4))\", 1),\n",
       " (\"(('docB', 4), ('docD', 4))\", 1),\n",
       " (\"(('docC', 4), ('docD', 4))\", 1),\n",
       " (\"(('docA', 4), ('docC', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 1),\n",
       " (\"(('docC', 4), ('docE', 3))\", 1),\n",
       " (\"(('docA', 4), ('docB', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 1),\n",
       " (\"(('docB', 4), ('docE', 3))\", 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda line: line.split('\\t')).flatMap(splitWords).reduceByKey(lambda x,y : x+y).flatMap(makeCompositeKey).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"(('docA', 4), ('docB', 4))\", 2),\n",
       " (\"(('docA', 4), ('docC', 4))\", 2),\n",
       " (\"(('docB', 4), ('docC', 4))\", 2),\n",
       " (\"(('docD', 4), ('docE', 3))\", 1),\n",
       " (\"(('docC', 4), ('docD', 4))\", 1),\n",
       " (\"(('docA', 4), ('docD', 4))\", 1),\n",
       " (\"(('docA', 4), ('docE', 3))\", 3),\n",
       " (\"(('docB', 4), ('docD', 4))\", 1),\n",
       " (\"(('docC', 4), ('docE', 3))\", 1),\n",
       " (\"(('docB', 4), ('docE', 3))\", 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda line: line.split('\\t')).flatMap(splitWords).reduceByKey(lambda x,y : x+y)\\\n",
    ".flatMap(makeCompositeKey).reduceByKey(lambda x,y : x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('docA - docB', 0.3333333333333333),\n",
       " ('docA - docC', 0.3333333333333333),\n",
       " ('docB - docC', 0.3333333333333333),\n",
       " ('docD - docE', 0.16666666666666666),\n",
       " ('docC - docD', 0.14285714285714285),\n",
       " ('docA - docD', 0.14285714285714285),\n",
       " ('docA - docE', 0.75),\n",
       " ('docB - docD', 0.14285714285714285),\n",
       " ('docC - docE', 0.16666666666666666),\n",
       " ('docB - docE', 0.16666666666666666)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda line: line.split('\\t')).flatMap(splitWords).reduceByKey(lambda x,y : x+y)\\\n",
    ".flatMap(makeCompositeKey).reduceByKey(lambda x,y : x+y).flatMap(jaccard).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*\\*\\*Scratch work ends here\\*\\*\\***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('docA - docE', 0.75),\n",
       " ('docA - docB', 0.3333333333333333),\n",
       " ('docA - docC', 0.3333333333333333),\n",
       " ('docB - docC', 0.3333333333333333),\n",
       " ('docD - docE', 0.16666666666666666),\n",
       " ('docC - docE', 0.16666666666666666),\n",
       " ('docB - docE', 0.16666666666666666),\n",
       " ('docC - docD', 0.14285714285714285),\n",
       " ('docA - docD', 0.14285714285714285),\n",
       " ('docB - docD', 0.14285714285714285)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Job - RUN THIS CELL AS IS\n",
    "result = data.map(lambda line: line.split('\\t')) \\\n",
    "             .flatMap(splitWords) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(makeCompositeKey) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(jaccard) \\\n",
    "             .takeOrdered(10, key=lambda x: -x[1])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "Now that you are comfortable with similarity metrics we turn to the main task in this assignment: Synonym Detection. As you saw in Question 3 the ability of our algorithm to detect words with similar meanings is highly dependent on our input text. Specifically, we need a large enough corpus of natural language that we can expose our algorithm to a realistic range of contexts in which in any given word might get used. Ideally, these 'contexts' would also provide enough signal to distinguish between words with similar semantic roles but different meaning. Finding such a corpus will be easier to accomplish for some words than others.\n",
    "\n",
    "For the main task in this portion of the homework you will use data from Google's n-gram corpus. This data is particularly convenient for our task because Google has already done the first step for us: they windowed over a large subset of the web and extracted all 5-grams. If you are interested in learning more about this dataset the original source is: http://books.google.com/ngrams/, and a large subset is available [here from AWS](https://aws.amazon.com/datasets/google-books-ngrams/). \n",
    "\n",
    "For this assignment we have provided a subset of the 5-grams data consisting of 191 files of approximately 10MB each. These files are available on dropbox at: https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea\n",
    "... below we provide the code to download these files. Please only use the provided data so that we can ensure consistent results from student to student.\n",
    "\n",
    "Each row in our dataset represents one of these 5 grams in the format:\n",
    "> `(ngram) \\t (count) \\t (pages_count) \\t (books_count)`\n",
    "\n",
    "__DISCLAIMER__: In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data.  Calculating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some similar terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  829M    0  829M    0     0   544k      0 --:--:--  0:25:59 --:--:--  851k\n"
     ]
    }
   ],
   "source": [
    "# download the zipped data folder - RUN THIS CELL AS IS\n",
    "!curl -L -o ngrams.zip https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n"
     ]
    }
   ],
   "source": [
    "# unzip the data files into the data directory - RUN THIS CELL AS IS\n",
    "!unzip -q ngrams.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global paths to full data folder and to the first file (which we'll use for testing)\n",
    "NGRAMS = PWD + '/data'\n",
    "F1_PATH = PWD + '/data/googlebooks-eng-all-5gram-20090715-0-filtered.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you develop your code you should use the following file to systems test each of your solutions before running it on the Google data. (Note: these are the 5-grams extracted from our two line Dickens corpus in Question 3... you should find that your Spark job results match the calculations we did \"by hand\").\n",
    "\n",
    "Test file: __`systems_test.txt`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting systems_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile systems_test.txt\n",
    "it was the best of\t1\t1\t1\n",
    "age of wisdom it was\t1\t1\t1\n",
    "best of times it was\t1\t1\t1\n",
    "it was the age of\t2\t1\t1\n",
    "it was the worst of\t1\t1\t1\n",
    "of times it was the\t2\t1\t1\n",
    "of wisdom it was the\t1\t1\t1\n",
    "the age of wisdom it\t1\t1\t1\n",
    "the best of times it\t1\t1\t1\n",
    "the worst of times it\t1\t1\t1\n",
    "times it was the age\t1\t1\t1\n",
    "times it was the worst\t1\t1\t1\n",
    "was the age of wisdom\t1\t1\t1\n",
    "was the best of times\t1\t1\t1\n",
    "was the age of foolishness\t1\t1\t1\n",
    "was the worst of times\t1\t1\t1\n",
    "wisdom it was the age\t1\t1\t1\n",
    "worst of times it was\t1\t1\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create a Spark RDD for each of these files so that they're easy to access throughout the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark RDDs for each dataset\n",
    "testRDD = sc.textFile(\"systems_test.txt\") \n",
    "f1RDD = sc.textFile(F1_PATH)\n",
    "dataRDD = sc.textFile(NGRAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peak at what each of these RDDs looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was the best of\\t1\\t1\\t1',\n",
       " 'age of wisdom it was\\t1\\t1\\t1',\n",
       " 'best of times it was\\t1\\t1\\t1',\n",
       " 'it was the age of\\t2\\t1\\t1',\n",
       " 'it was the worst of\\t1\\t1\\t1',\n",
       " 'of times it was the\\t2\\t1\\t1',\n",
       " 'of wisdom it was the\\t1\\t1\\t1',\n",
       " 'the age of wisdom it\\t1\\t1\\t1',\n",
       " 'the best of times it\\t1\\t1\\t1',\n",
       " 'the worst of times it\\t1\\t1\\t1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A BILL FOR ESTABLISHING RELIGIOUS\\t59\\t59\\t54',\n",
       " 'A Biography of General George\\t92\\t90\\t74',\n",
       " 'A Case Study in Government\\t102\\t102\\t78',\n",
       " 'A Case Study of Female\\t447\\t447\\t327',\n",
       " 'A Case Study of Limited\\t55\\t55\\t43',\n",
       " \"A Child's Christmas in Wales\\t1099\\t1061\\t866\",\n",
       " 'A Circumstantial Narrative of the\\t62\\t62\\t50',\n",
       " 'A City by the Sea\\t62\\t60\\t49',\n",
       " 'A Collection of Fairy Tales\\t123\\t117\\t80',\n",
       " 'A Collection of Forms of\\t116\\t103\\t82']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A CATALOGUE OF THE PATHOLOGICAL\\t90\\t90\\t86',\n",
       " 'A CT scan shows a\\t208\\t204\\t123',\n",
       " 'A Case of Frustrated Take\\t94\\t94\\t77',\n",
       " 'A Catalogue of Books and\\t272\\t270\\t239',\n",
       " 'A Celebration of the First\\t129\\t117\\t89',\n",
       " 'A Century of Failed School\\t401\\t400\\t257',\n",
       " 'A Christian Church is a\\t46\\t46\\t46',\n",
       " \"A Citizen's Right to Know\\t71\\t69\\t57\",\n",
       " 'A Clinical Study of Treatment\\t43\\t43\\t40',\n",
       " 'A Commentary on the Epistle\\t2870\\t2645\\t1475']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: N-gram EDA part 1 (words)\n",
    "\n",
    "Before starting our synonym-detection, let's get a sense for this data. As you saw in questions 3 and 4 the size of the vocabulary will impact the amount of computation we have to do. Write a Spark job that will accomplish the three tasks below as efficiently as possible. (No credit will be awarded for jobs that sort or subset after calling `collect()`-- use the framework to get the minimum information requested). As you develop your code, systems test each job on the provided file with Dickens ngrams, then on a single file from the Ngram dataset before running the full analysis.\n",
    "\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) code:__ Write a Spark job to retrieve:\n",
    "  * The number of unique words that appear in the data. (i.e. size of the vocabulary) \n",
    "  * A list of the top 10 words & their counts.\n",
    "  * A list of the bottom 10 words & their counts.  \n",
    "  \n",
    "  __`NOTE  1:`__ _don't forget to lower case the ngrams before extracting words._  \n",
    "  __`NOTE  2:`__ _don't forget to take in to account the number of occurances of each ngram._  \n",
    "  __`NOTE  3:`__ _to make this code more reusable, the `EDA1` function code base uses a parameter 'n' to specify the number of top/bottom words to print (in this case we've requested 10)._\n",
    "\n",
    "\n",
    "* __b) short response:__ Given the vocab size you found in part a, how many potential synonym pairs could we form from this corpus? If each term's stripe were 1000 words long, how many individual 'postings' tuples would we need to shuffle inorder to form the inverted indices? Show and briefly explain your calculations for each part of this question. [__`HINT:`__ see your work from q4 for a review of these concepts.]\n",
    "\n",
    "* __c) short response:__ What do you notice about the most frequent words, how usefull will these top words be in synonym detection? Explain.\n",
    "\n",
    "* __d) short response:__ What do you notice/infer about the least frequent words, how reliable should we expect the detected 'synonyms' for the bottom words to be? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "\n",
    "> __b)__ Given the vocab size of 269339, the number of potential synonym pairs is 269339x269338/2 = 36271613791 pairs. In the algorithm Q4, one tuple was created for each word in the strip to form the inverted indice.  For each word that is the key, we would have 1000 tuples corresponding to the pair of word key and a count of 1.  Given that the vocab size of 269339, we would need to shuffle a total of 2693390 individual tuples, which is a lot less than the number of pairs.\n",
    "\n",
    "> __c)__ The most frequent words are common words that are not useful in synonym detection.  Most of these words are words that appear in a large number of sentences and thus do not contribute to the meaning of vocabulary words.  As we had done in #2, the removal of common words, which are also referred to as stop words, will help with similarity calculations that better match our expectations in interpretation of the text.  Therefore, the most common words should be labeled as stop words so that they do not contribute tot he vocabulary space.    \n",
    "\n",
    "> __d)__ The least frequent words all of a count of 40 and they appear to be extremely uncommon, either as very rarely used vocabulary in English or they are proper names of locations, people (example: schwetzingen).  Given their extremely low frequencies and the very high frequencies of stop words (most common words), I would expect detected synonyms for the bottom words to be very more unreliable the smaller the size n of n-gram used for each word's stripe.  If we are using only 5-grams for instance, the sparseness of the least frequent words would lead to similarity metrics that are close to 0, since the likelihood of finding these words in the same 5 word neighborhood of actual synonyms is rare, given their own rareness.  If we increase n to much larger, we may be increase the likelihood fo finding synonyms in the same neighborhood, thereby increasing the similarity metrics and makes the detection more reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - write your spark job here \n",
    "def parse(doc):\n",
    "    '''helper function to parse documents and return word counts'''\n",
    "    \n",
    "    #parse row\n",
    "    ngram, ngram_count, pages_count, books_count =  doc.lower().split('\\t')\n",
    "    #parse ngram\n",
    "    words = ngram.split(' ')\n",
    "    #the count returned for each word is actually the ngram count, or 1*ngram count\n",
    "    return ([(word, int(ngram_count)) for word in words])\n",
    "\n",
    "    \n",
    "\n",
    "def EDA1(rdd, n):\n",
    "    total, top_n, bottom_n = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    \n",
    "    unique_words = rdd.flatMap(parse) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .cache()\n",
    "    \n",
    "    total = unique_words.count()\n",
    "    top_n = unique_words.takeOrdered(n, key=lambda x: -x[1])\n",
    "    bottom_n = unique_words.takeOrdered(n, key=lambda x: x[1])\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return total, top_n, bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.9151229858398438 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(testRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     was         17 |    foolishness   1\n",
      "      of         17 |           best   4\n",
      "     the         17 |          worst   5\n",
      "      it         16 |         wisdom   5\n",
      "   times         10 |            age   8\n",
      "     age          8 |          times  10\n",
      "   worst          5 |             it  16\n",
      "  wisdom          5 |            was  17\n",
      "    best          4 |             of  17\n",
      "foolishness          1 |            the  17\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.6642043590545654 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run a single file (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(f1RDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 36353\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     the   27691943 |    stakeholder  40\n",
      "      of   18590950 |          kenny  40\n",
      "      to   11601757 |         barnes  40\n",
      "      in    7470912 |         arnall  40\n",
      "       a    6926743 |     buonaparte  40\n",
      "     and    6150529 |       puzzling  40\n",
      "    that    4077421 |             hd  40\n",
      "      is    4074864 |        corisca  40\n",
      "      be    3720812 |       cristina  40\n",
      "     was    2492074 |         durban  40\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 343.57852387428284 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run full analysis (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(dataRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 269339\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     the 5490815394 |   schwetzingen  40\n",
      "      of 3698583299 |           cras  40\n",
      "      to 2227866570 |       parcival  40\n",
      "      in 1421312776 |          porti  40\n",
      "       a 1361123022 |    scribbler's  40\n",
      "     and 1149577477 |      washermen  40\n",
      "    that  802921147 |    viscerating  40\n",
      "      is  758328796 |         mildes  40\n",
      "      be  688707130 |      scholared  40\n",
      "      as  492170314 |       jaworski  40\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__  \n",
    "_(bottom words might vary a little due to ties)._\n",
    "<table>\n",
    "<th>Top Words</th>\n",
    "<th>Bottom Words</th>\n",
    "<tr><td><pre>\n",
    "the 5490815394\n",
    "of 3698583299\n",
    "to 2227866570\n",
    "in 1421312776\n",
    "a 1361123022\n",
    "and 1149577477\n",
    "that  802921147\n",
    "is  758328796\n",
    "be  688707130\n",
    "as  492170314\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "foretastes  40\n",
    "parcival  40\n",
    "schwetzingen  40\n",
    "cras  40\n",
    " scholared  40\n",
    "scribbler's  40\n",
    "washermen  40\n",
    "viscerating  40\n",
    "unmurmuring  40\n",
    "mildes  40\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: N-gram EDA part 2 (co-occurrences)\n",
    "\n",
    "The computational complexity of synonym analysis depends not only on the number of words, but also on the number of co-ocurrences each word has. In this question you'll take a closer look at that aspect of our data. As before, please test each job on small \"systems test\" (Dickens ngrams) file and on a single file from the Ngram dataset before running the full analysis.\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) code:__ Write a spark job that computes:\n",
    "  * the number of unique neighbors (i.e. 5-gram co-occuring words) for each word in the vocabulary. \n",
    "  * the top 10 words with the most \"neighbors\"\n",
    "  * the bottom 10 words with least \"neighbors\"\n",
    "  * a random sample of 1% of the words' neighbor counts  \n",
    "  __`NOTE:`__ for the last item, please return only the counts and not the words -- we'll go on to use these in a plotting function that expects a list of integers.\n",
    "\n",
    "\n",
    "* __b) short response:__ Use the provided code to plot a histogram of the sampled list from `a`. Comment on the distribution you observe. How will this distribution affect our synonym detection analysis?\n",
    "\n",
    "* __c) code + short response:__ Write a Spark Job to compare the top/bottom words from Q5 and from part a. Specifically, what % of the 1000 most/least neighbors words also appear in the list of 1000 most/least frequent words. [__`NOTE:`__ _technically these lists are short enough to comparing in memory on your local machine but please design your Spark job as if we were potentially comparing much larger lists._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "\n",
    "> __b)__ In the sample of 27010, it looks like the majority of words, as many as 24000, have 100 neighbors or less. There are a few hundred words with up to 1000 neighbors.  The number words with more than 1000 neighbors appear neglible with the sample of 27010.  In terms of our analysis with synonym detection, this will help us determine the feature space.  Since similarity metrics are determined by neighboring word overlaps between two words, our intution at first is to analyze as many neighbors as we collect.  However allowing an extremely large feature space where we allow the collection of unlimited neighbors may lead to exponentially higher complexity without better gains.  Given that the number of words with more than 1000 neighbors is negligible, perhaps it may make sense to limit the feature vectors to have length of 1000, allowing us to have a more efficient analysis.\n",
    "\n",
    "> __c)__ 88% of the list of 1000 words with the most neighbors also appear in the list 1000 most frequent words. 1.9% of the 1000 words with the least neighbors appear in the list of 1000 least frequent words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2(doc):\n",
    "    '''helper function to parse documents and return unique neighbors in the n-gram as a set'''\n",
    "    \n",
    "    #parse row\n",
    "    ngram, ngram_count, pages_count, books_count =  doc.lower().split('\\t')\n",
    "    #parse ngram\n",
    "    words = ngram.split(' ')\n",
    "    #for each word, we return all of the unique words in the n-gram\n",
    "    #wedo not emit a key-value pair for words with 0 neighbors\n",
    "    return ([(word, set(words)-set([word])) for word in words if len(set(words)) > 1])\n",
    "\n",
    "    \n",
    "def set_update(set_x, set_y):\n",
    "    '''ensures that the union of two sets is properly returned\n",
    "    - set_x.update(set_y) returns only None rather than the resulting set\n",
    "    - function returns the actual union, which set_x'''\n",
    "    set_x.update(set_y)\n",
    "    return set_x\n",
    "\n",
    "# part a - spark job\n",
    "def EDA2(rdd,n):\n",
    "    top_n, bottom_n, sampled_counts = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    \n",
    "    unique_words_nbrs = rdd.flatMap(parse2) \\\n",
    "             .reduceByKey(lambda x,y : set_update(x,y)) \\\n",
    "             .map(lambda x: (x[0], len(x[1]))) \\\n",
    "             .cache()\n",
    "    \n",
    "    top_n = unique_words_nbrs.takeOrdered(n, key=lambda x: -x[1])\n",
    "    bottom_n = unique_words_nbrs.takeOrdered(n, key=lambda x: x[1])\n",
    "    sampled_counts = unique_words_nbrs.sample(withReplacement = False, fraction=0.1).map(lambda x: x[1]).collect()\n",
    "\n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return top_n, bottom_n, sampled_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.591031551361084 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - systems test (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(testRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         was        9 |     foolishness    4\n",
      "          of        9 |            best    5\n",
      "         the        9 |           worst    5\n",
      "          it        8 |          wisdom    5\n",
      "         age        7 |             age    7\n",
      "       times        7 |           times    7\n",
      "        best        5 |              it    8\n",
      "       worst        5 |             was    9\n",
      "      wisdom        5 |              of    9\n",
      " foolishness        4 |             the    9\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.414033651351929 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - single file test (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(f1RDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         the    25548 |              vo    1\n",
      "          of    22496 |           pizza    2\n",
      "         and    16489 |      noncleaved    2\n",
      "          to    14249 |        premiers    2\n",
      "          in    13891 |        enclaves    2\n",
      "           a    13045 |   selectiveness    2\n",
      "        that     8011 |           trill    2\n",
      "          is     7947 |             gem    2\n",
      "        with     7552 |            hoot    2\n",
      "          by     7400 |     palpitation    2\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 785.657482624054 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - full data (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(dataRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         the   164982 |          cococo    1\n",
      "          of   155708 |            inin    1\n",
      "         and   132814 |        charuhas    1\n",
      "          in   110615 |         ooooooo    1\n",
      "          to    94358 |           iiiii    1\n",
      "           a    89197 |          iiiiii    1\n",
      "          by    67266 |             cnj    1\n",
      "        with    65127 |            choh    1\n",
      "        that    61174 |             neg    1\n",
      "          as    60652 |      cococococo    1\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part a expected output:__  \n",
    "_(bottom words might vary a little due to ties)._\n",
    "<table>\n",
    "<th>Most Neighbors</th>\n",
    "<th>Least Neighbors</th>\n",
    "<tr><td><pre>\n",
    "the   164982 \n",
    "of   155708 \n",
    "and   132814 \n",
    "in   110615 \n",
    "to    94358 \n",
    "a    89197\n",
    "by    67266\n",
    "with    65127\n",
    "that    61174\n",
    "as    60652\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "cococo    1\n",
    "inin    1\n",
    "charuhas    1\n",
    "ooooooo    1\n",
    "iiiii    1\n",
    "iiiiii    1\n",
    "cnj    1\n",
    "choh    1\n",
    "neg    1\n",
    "cococococo    1\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ _before running the plotting code below, make sure that the variable_ `sample_counts` _points to the list generated in_ `part a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: we'll exclude the 84 words with more than 6000 nbrs in this 26720 count sample.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# part b - plot histogram (RUN THIS CELL AS IS - feel free to modify format)\n",
    "\n",
    "# removing extreme upper tail for a better visual\n",
    "counts = np.array(sample_counts)[np.array(sample_counts) < 6000]\n",
    "t = sum(np.array(sample_counts) > 6000)\n",
    "n = len(counts)\n",
    "print(\"NOTE: we'll exclude the %s words with more than 6000 nbrs in this %s count sample.\" % (t,n))\n",
    "\n",
    "# set up figure\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "# plot regular hist\n",
    "ax1.hist(counts, bins=50)\n",
    "ax1.set_title('Freqency of Number of Co-Words', color='0.1')\n",
    "ax1.set_facecolor('0.9')\n",
    "ax1.tick_params(axis='both', colors='0.1')\n",
    "ax1.grid(True)\n",
    "\n",
    "# plot log scale hist\n",
    "ax2.hist(counts, bins=50)\n",
    "ax2.set_title('(log)Freqency of Number of Co-Words', color='0.1')\n",
    "ax2.set_facecolor('0.9')\n",
    "ax2.tick_params(axis='both', colors='0.1')\n",
    "ax2.grid(True)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - spark job\n",
    "def compareRankings(rdd1, rdd2):\n",
    "    percent_overlap = None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    \n",
    "    percent_overlap = rdd1.keys().intersection(rdd2.keys()).count()/rdd1.count()*100\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return percent_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 1000 words with most neighbors, 80.0 percent are also in the list of 1000 most frequent words.\n",
      "Of the 1000 words with least neighbors, 100.0 percent are also in the list of 1000 least frequent words.\n"
     ]
    }
   ],
   "source": [
    "# part c - get lists for comparison (RUN THIS CELL AS IS...)\n",
    "# (... then change 'testRDD' to 'f1RDD'/'dataRDD' when ready)\n",
    "total, topWords, bottomWords = EDA1(testRDD, 5)\n",
    "topNbrs, bottomNbrs, sample_counts = EDA2(testRDD, 5)\n",
    "twRDD = sc.parallelize(topWords)\n",
    "bwRDD = sc.parallelize(bottomWords)\n",
    "tnRDD = sc.parallelize(topNbrs)\n",
    "bnRDD = sc.parallelize(bottomNbrs)\n",
    "top_overlap = compareRankings(tnRDD, twRDD)\n",
    "bottom_overlap = compareRankings(bnRDD,bwRDD)\n",
    "print(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\n",
    "print(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 1000 words with most neighbors, 87.7 percent are also in the list of 1000 most frequent words.\n",
      "Of the 1000 words with least neighbors, 5.2 percent are also in the list of 1000 least frequent words.\n"
     ]
    }
   ],
   "source": [
    "# part c - get lists for comparison (RUN THIS CELL AS IS...)\n",
    "# for f1RDD \n",
    "total, topWords, bottomWords = EDA1(f1RDD, 1000)\n",
    "topNbrs, bottomNbrs, sample_counts = EDA2(f1RDD, 1000)\n",
    "twRDD = sc.parallelize(topWords)\n",
    "bwRDD = sc.parallelize(bottomWords)\n",
    "tnRDD = sc.parallelize(topNbrs)\n",
    "bnRDD = sc.parallelize(bottomNbrs)\n",
    "top_overlap = compareRankings(tnRDD, twRDD)\n",
    "bottom_overlap = compareRankings(bnRDD,bwRDD)\n",
    "print(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\n",
    "print(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 1000 words with most neighbors, 88.0 percent are also in the list of 1000 most frequent words.\n",
      "Of the 1000 words with least neighbors, 1.9 percent are also in the list of 1000 least frequent words.\n"
     ]
    }
   ],
   "source": [
    "# part c - get lists for comparison (RUN THIS CELL AS IS...)\n",
    "# for dataRDD\n",
    "total, topWords, bottomWords = EDA1(dataRDD, 1000)\n",
    "topNbrs, bottomNbrs, sample_counts = EDA2(dataRDD, 1000)\n",
    "twRDD = sc.parallelize(topWords)\n",
    "bwRDD = sc.parallelize(bottomWords)\n",
    "tnRDD = sc.parallelize(topNbrs)\n",
    "bnRDD = sc.parallelize(bottomNbrs)\n",
    "top_overlap = compareRankings(tnRDD, twRDD)\n",
    "bottom_overlap = compareRankings(bnRDD,bwRDD)\n",
    "print(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\n",
    "print(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Basis Vocabulary & Stripes\n",
    "\n",
    "Every word that appears in our data is a potential feature for our synonym detection analysis. However as we've discussed, some are likely to be more useful than others. In this question, you'll choose a judicious subset of these words to form our 'basis vocabulary' (i.e. feature set). Practically speaking, this means that when we build our stripes, we are only going to keep track of when a term co-occurs with one of these basis words. \n",
    "\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ Suppose we were deciding between two different basis vocabularies: the 1000 most frequent words or the 1000 least frequent words. How would this choice impact the quality of the synonyms we are able to detect? How does this choice relate to the ideas of 'overfitting' or 'underfitting' a training set?\n",
    "\n",
    "* __b) short response:__ If we had a much larger dataset, computing the full ordered list of words would be extremely expensive. If we need to none-the-less get an estimate of word frequency in order to decide on a basis vocabulary (feature set), what alternative strategy could we take?\n",
    "\n",
    "* __c) code:__ Write a spark job that does the following:\n",
    "  * tokenizes, removes stopwords and computes a word count on the ngram data\n",
    "  * subsets the top 10,000 words (these are the terms we'll consider as potential synonyms)\n",
    "  * subsets words 9,000-9,999 (this will be our 1,000 word basis vocabulary)    \n",
    "  (to put it another way - of the top 10,000 words, the bottom 1,000 form the basis vocabulary)\n",
    "  * saves the full 10K word list and the 1K basis vocabulary to file for use in `d`.  \n",
    "  \n",
    "  __NOTE:__ _to ensure consistency in results please use only the provided list of stopwords._  \n",
    "  __NOTE:__ _as always, be sure to test your code on small files as you develop it._  \n",
    "\n",
    "* __d) code:__ Write a spark job that builds co-occurrence stripes for the top 10K words in the ngram data using the basis vocabulary you developed in `part c`. This job/function, unlike others so far, should return an RDD (which we will then use in q8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__ The 1000 most frequent words would be a poor basis.  As we have seen with freqent words included in analysis with number 4, frequent words do not contribute much to the meaning of their neighbors.  Given their prevalence, they end up being neighbors of many other words.  Thus training with a basis of the 1000 most frequent words would lead to underfitting, a model that cannot capture word meanings correctly because the features vectors are not a good representation of the data needed to calculate similarities reliably.  This is characterized as underfitting because the data trends are not captured by the model and the prediction quality is poor.  Using the 1000 least frequent words would also be a poor basis.  The 1000 least frequent words are extremely sparce and likely do not exist as the neighbors of most words in the corpus. For most words we analysis, the features vectors constructed from a basis of 1000 least frequent words would be mostly 0's.  This leads us to build a model with very few data points that then cannot generalize well to new data sets, which would lead to overfitting.\n",
    "\n",
    "> __b)__ In the event of an extremely large corpus, getting am estimate for freqencies can be done through sampling followed by extrapolation.  We can take multiple smaller samples from the corpus and perform word counts on these samples and perhaps apply some sort of averaging calculations at the end.  Taking mutilple samples will also allow us to see which words, if any, lead to wildly different counts across samples. For extrapolation, we can then perform a transformation to all counts where we multiply them by factor that is equal to the size of the corpus divided by the sample size.  This averaging a d extrapolation can only be done if we find that sample freqencies for the same words do not vary greatly across samples. Otherwise, more complicated weighted averaging will need to be performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - provided stopwords (RUN THIS CELL AS IS)\n",
    "STOPWORDS =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 'should', 'can', 'now', 'will', 'just', \n",
    "              'would', 'could', 'may', 'must', 'one', 'much', \"it's\",\n",
    "              \"can't\", \"won't\", \"don't\", \"shouldn't\", \"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - write your spark job here \n",
    "def parse_words(doc, stopwords):\n",
    "    '''helper function to parse documents and return a partial word count\n",
    "    words in the list of stopwords are excluded'''\n",
    "    \n",
    "    #parse row\n",
    "    ngram, ngram_count, pages_count, books_count =  doc.lower().split('\\t')\n",
    "    words = ngram.split(' ')\n",
    "    #return words and count = 1*n_gram_count but exclude stopwords\n",
    "    return[(w, int(ngram_count)) for w in words if w not in stopwords]\n",
    "\n",
    "# part c - spark job\n",
    "def get_vocab(rdd, n_total, n_basis):\n",
    "    vocab, basis = None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    stopwords = sc.broadcast(set(STOPWORDS))\n",
    "    \n",
    "    vocab = rdd.flatMap(lambda x: parse_words(x, stopwords.value)) \\\n",
    "               .reduceByKey(lambda x,y : x+y) \\\n",
    "               .sortBy(lambda x: x[1], ascending = False) \\\n",
    "               .keys() \\\n",
    "               .take(n_total) \n",
    "    \n",
    "\n",
    "    basis = vocab[-n_basis:]\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return vocab, basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - run your job (replace 'testRDD' with 'f1RDD'/'dataRDD' when ready)\n",
    "VOCAB, BASIS = get_vocab(f1RDD, 10000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - save to file (RUN THIS CELL AS IS)\n",
    "with open(\"vocabulary_f1RDD.txt\", \"w\") as file:\n",
    "    file.write(str(VOCAB))\n",
    "with open(\"basis_f1RDD.txt\", \"w\") as file:\n",
    "    file.write(str(BASIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - for dataRDD\n",
    "VOCAB, BASIS = get_vocab(dataRDD, 10000, 1000)\n",
    "with open(\"vocabulary.txt\", \"w\") as file:\n",
    "    file.write(str(VOCAB))\n",
    "with open(\"basis.txt\", \"w\") as file:\n",
    "    file.write(str(BASIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - spark job\n",
    "\n",
    "def set_intersection_update(set_x,set_y):\n",
    "    '''ensures that the intersection of two sets is properly returned\n",
    "    - set_x.intersection_update(set_y) returns only None rather than the resulting set\n",
    "    - function returns the actual intersection, which set_x'''\n",
    "    \n",
    "    set_x.intersection_update(set_y)\n",
    "    return set_x\n",
    "\n",
    "def parse_words_vocab(doc, vocab):\n",
    "    '''helper function parses documents and returns a unique words in each ngram\n",
    "    only words in the vocabulary are included'''\n",
    "    #parse row\n",
    "    ngram, ngram_count, pages_count, books_count =  doc.lower().split('\\t')\n",
    "    #parse ngram and turn to set\n",
    "    words = set(ngram.split(' '))\n",
    "    return set_intersection_update(words, vocab)\n",
    "\n",
    "def stripes(words):\n",
    "    '''helper function that takes a set of n-gram words\n",
    "    produces a partial stripe of unique neighbors\n",
    "    only words in basis are included'''\n",
    "    return [(word, words-set([word])) for word in words if len(words) > 0]\n",
    "    \n",
    "\n",
    "def buildStripes(rdd, vocab, basis):\n",
    "    stripesRDD = None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    \n",
    "    vocab = sc.broadcast(set(vocab))\n",
    "    basis = sc.broadcast(set(basis))\n",
    "    \n",
    "\n",
    "    stripesRDD = rdd.map(lambda x: parse_words_vocab(x, vocab.value)) \\\n",
    "                    .flatMap(stripes) \\\n",
    "                    .map(lambda x: (x[0], set_intersection_update(x[1], basis.value)))\\\n",
    "                    .filter(lambda x: len(x[1]) > 0) \\\n",
    "                    .reduceByKey(lambda x,y : set_update(x,y)) \\\n",
    "                    .cache()\n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return stripesRDD\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.29831600189208984 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run your systems test (RUN THIS CELL AS IS)\n",
    "VOCAB, BASIS = get_vocab(testRDD, 10, 10)\n",
    "testStripesRDD = buildStripes(testRDD, VOCAB, BASIS)\n",
    "start = time.time()\n",
    "testStripesRDD.top(10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', {'times'}),\n",
       " ('wisdom', {'age'}),\n",
       " ('times', {'age', 'best', 'worst'}),\n",
       " ('foolishness', {'age'}),\n",
       " ('best', {'times'}),\n",
       " ('age', {'foolishness', 'times', 'wisdom'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testStripesRDD.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.4849853515625e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run your single file test (RUN THIS CELL AS IS)\n",
    "VOCAB = ast.literal_eval(open(\"vocabulary.txt\", \"r\").read())\n",
    "BASIS = ast.literal_eval(open(\"basis.txt\", \"r\").read())\n",
    "f1StripesRDD = buildStripes(f1RDD, VOCAB, BASIS)\n",
    "f1StripesRDD.top(5)\n",
    "start = time.time()\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zenith', {'meridian'}),\n",
       " ('youth', {'affiliated', 'frustrated', 'illusions', 'mould'}),\n",
       " ('younger', {'careers'}),\n",
       " ('young', {'bees', 'hay', 'pink', 'scrap', 'undergoing', 'warrior', 'wax'}),\n",
       " ('york', {'designer', 'fifties', 'norway', 'tribune'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1StripesRDD.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - run the full analysis (RUN THIS CELL AS IS)\n",
    "VOCAB = ast.literal_eval(open(\"vocabulary.txt\", \"r\").read())\n",
    "BASIS = ast.literal_eval(open(\"basis.txt\", \"r\").read())\n",
    "stripesRDD = buildStripes(dataRDD, VOCAB, BASIS).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zones\n",
      "['adhesion', 'warmer', 'environments', 'buffer', 'gaza', 'subdivided', 'parks', 'residential', 'uppermost', 'remotest', 'localities', 'saturation']\n",
      "-------\n",
      "zone\n",
      "['diffuse', 'trigger', 'americas', 'poorly', 'persia', 'subdivided', 'tribal', 'auxiliary', 'unusually', 'alaska', 'sandy', 'saturation', 'vomiting', 'flexor', 'parked', 'intervening', 'cartilage', 'inorganic', 'uppermost', 'masculine', 'avoidance', 'cracks', 'guides', 'ie', 'traversed', 'au', 'originate', 'accumulate', 'southeastern', 'illuminated', 'assisting', 'articular', 'atlas', 'turbulent', 'narrower', 'penis', 'buffer', 'glowing', 'defines', 'penetrating', 'officially', 'residential', 'fibrous', 'transitional', 'contamination', 'excitation']\n",
      "-------\n",
      "zinc\n",
      "['dysfunction', 'burns', 'pancreas', 'hydroxide', 'insoluble', 'weighing', 'wasting', \"alzheimer's\", 'transcription', 'diamond', 'leukemia', 'dietary', 'phosphorus', 'coating', 'dipped', 'radioactive', 'metallic', 'ammonium']\n",
      "-------\n",
      "Wall time: 371.35845041275024 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - take a look at a few stripes (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "for wrd, stripe in stripesRDD.top(3):\n",
    "    print(wrd)\n",
    "    print(list(stripe))\n",
    "    print('-------')\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - save your full stripes to file for ease of retrival later... (OPTIONAL)\n",
    "stripesRDD.saveAsTextFile(PWD + '/stripes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Synonym Detection\n",
    "\n",
    "We're now ready to perform the main synonym detection analysis. In the tasks below you will compute cosine, jaccard, dice and overlap similarity measurements for each pair of words in our vocabulary and then sort your results to find the most similar pairs of words in this dataset. __`IMPORTANT:`__ When you get to the sorting step please __sort on cosine similarity__ only, so that we can ensure consistent results from student to student. \n",
    "\n",
    "Remember to test each step of your work with the small files before running your code on the full dataset. This is a computationally intense task: well designed code can be the difference between a 20min job and a 2hr job. __`NOTE:`__ _as you are designing your code you may want to review questions 3 and 4 where we modeled some of the key pieces of this analysis._\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) short response:__ In question 7 you wrote a function that would create word stripes for each `term` in our vocabulary. These word stripes are essentially an 'embedded representation' of the `term`'s meaning. What is the 'feature space' for this representation? (i.e. what are the features of our 1-hot encoded vectors?). What is the maximum length of a stripe?\n",
    "\n",
    "* __b) short response:__ Remember that we are going to treat these stripes as 'documents' and perform similarity analysis on them. The first step is to emit postings which then get collected to form an 'inverted index.' How many rows will there be in our inverted index? Explain.\n",
    "\n",
    "* __c) short response:__ In the demo from question 2, we were able to compute the cosine similarity directly from the stripes (we did this using their vector form, but could have used the list instead). So why do we need the inverted index? (__`HINT:`__ _see your answer to Q4a & Q4b_)\n",
    "\n",
    "* __d) code:__ Write a spark job that does the following:\n",
    "  * loops over the stripes from Q7 and emits postings for the `term` (_remember stripe = document_)\n",
    "  * aggregates the postings to create an inverted index\n",
    "  * loops over all pairs of `term`s that appear in the same inverted index and emits co-occurrence counts\n",
    "  * aggregates co-occurrences\n",
    "  * uses the counts (along with the accompanying information) to compute the cosine, jacard, dice and overlap similarity metrics for each pair of words in the vocabulary \n",
    "  * retrieve the top 20 and bottom 20 most/least similar pairs of words\n",
    "  * also returned the cached sorted RDD for use in the next question  \n",
    "  __`NOTE 1`:__ _Don't forget to include the stripe length when you are creating the postings & co-occurrence pairs. A composite key is the way to go here._  \n",
    "  __`NOTE 2`:__ _Please make sure that your final results are sorted according to cosine similarity otherwise your results may not match the expected result & you will be marked wrong._\n",
    "  \n",
    "* __e) code:__ Comment on the quality of the \"synonyms\" your analysis comes up with. Do you notice anything odd about these pairs of words? Discuss at least one idea for how you might go about improving on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "> __a)__ The feature space for this representation is the 1000 words that are in the basis.  To calculate similarity of words in our vocabulary, we are taking into consideration of neighboring words of each vocabulary term only if the neighbor is in the 1000 basis.  Therefore, the basis constitutes the feature space and each features vector, if we use one-hot encoded notation would have 1000 elements where a 1 indicates that a word from the basis is a neighbor of the term and 0 indicates the basis word is not.  In our dense representation, the stripes only contain the basis words for which the one-hot encoded vector entry is 1.  Given that the limit of the basis is 1000 words, and we only collect unique neighbors in each stripe, the maximum length of any stripe is 1000.\n",
    "\n",
    "> __b)__ The inverted index is created by to have the basis term as the key and a list of vocabulary words and stripe length tuples as the value.  This is inverted from the input, which has vocabulary words as the key and the basis terms as the values in the stripes.  When the inverted index is first created, there is one row for each occurence of a basis word as a stripe.  So the number of rows is the stripe length of every vocabulary word, summed over all 10000 vocabulary words.  After a reduce by key is performed on the inverted index, where all of the postings with the same basis term are collected, the number of rows of the inverted index is at most 1000, and would equal 1000 if every word in the feature space appeard as a neighbor at least one time. \n",
    "\n",
    "> __c)__ In question 2, we didn't use inverted index because we committed the entire one-hot encoded matrix to memory, and then we compared all possible combinations of terms.  We need the inverted index for this question because we do not have unlimited memory or unlimited resources to shuffle our data.  As a corpus gets larger, storing our entire vocabulary words and stripes data onto disk is very costly and maybe not possible.  In addition, comparing all word pairs will require 10000x9999/2 = 49995000 comparisons. With inverted index, we are able to generate all possible pairs of vocabulary words from the stripes data without incurring too much resources, since we only commit each strip to memory temporarily to generate the composite keys of pairs.  Furthermore, we creating much fewer comparisons rows with inverted syntax, since prior to reducing the inverted index will have most 1000x10000=10000000 rows, which is still 1/5 the number of rows as if we had not used inverted index.  In actual practice, the inverted index method produces much fewer rows and therefore less data for shuffling than 10000 since most words have 100 neighbors or less and very few words actually have 1000 neighbors. \n",
    "\n",
    "> __e)__ The most similar words have really high similarity scores, but many of the pairs are actually not synonyms.  Also the same words appear in many of the pairs, in particular, time, well, first and made. The pairs that appear are less synonym but words that are likely to appear together in the same sentence.  Therefore I would say that while the metrics are close to one, they are not super reliable in terms of finding true synonyms. One of the things I would change about the analysis in order to improve the results is to choose a better set of 1000 words for the features space.  While I think that choosing 1000 words from the middle of the most frequent words rankings much better than choosing the first 1000 or the last 1000, I do question if pulling 1000 words ranked 9000-10000 is a bit too haphazard.  While it is not wise to use only words in the first 1000 as the feature space, there might be some words in the first 1000 that are quite powerful in contributing to meanings of their neighbors.  The same can be said for words in the last 1000.  We are also missing useful words that are ranked from 1000-9000 or words that are immediately past the 10,000 cutoff.  Therefore, if we wanted to create a better basis  for improved results, but limit to 1000 words, we may want to consider randomly retriving words from various freqency rankings.  However, the number of words we retrive from each freqency ranking may be different depending on how close we are to the extremes.  That is to say, we may want to retrive words in the following way - 10 from ranking 1-1000, 20 from ranking 1000-2000, .... , 100 from ranking 9000-10000. In this way, we ensure that we pick the most words for the basis from the middle of the frequency rankings but we also do not miss words that might still be able to contribute to the meaning of their neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for pretty printing (RUN THIS CELL AS IS)\n",
    "def displayOutput(lines):\n",
    "    template = \"{:25}|{:6}, {:7}, {:7}, {:5}\"\n",
    "    print(template.format(\"Pair\", \"Cosine\", \"Jaccard\", \"Overlap\", \"Dice\"))\n",
    "    for pair, scores in lines:\n",
    "        scores = [round(s,4) for s in scores]\n",
    "        print(template.format(pair, *scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`TIP:`__ Feel free to define helper functions within the main function to help you organize your code. Readability is important! Eg:\n",
    "```\n",
    "def similarityAnlysis(stripesRDD):\n",
    "    \"\"\"main docstring\"\"\"\n",
    "    \n",
    "    simScoresRDD, top_n, bottom_n = None, None, None\n",
    "    \n",
    "    ############ YOUR CODE HERE ###########\n",
    "    def helper1():\n",
    "        \"\"\"helper docstring\"\"\"\n",
    "        return x\n",
    "        \n",
    "    def helper2():\n",
    "        \"\"\"helper docstring\"\"\"\n",
    "        return x\n",
    "        \n",
    "    # main spark job starts here\n",
    "    \n",
    "        ...etc\n",
    "    ############ (END) YOUR CODE ###########\n",
    "    return simScoresRDD, top_n, bottom_n\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - write your spark job in the space provided\n",
    "\n",
    "###Please note, to allow better accesbility for testing, I placed my helper functions outside of the similarityAnalysis() function\n",
    "def splitStripe(pair):\n",
    "    '''Takes each vocabulary word and splits the stripe words to create postings'''\n",
    "    term, stripe = pair\n",
    "    \n",
    "    for w in stripe:\n",
    "        yield (w, [(term,len(stripe))])\n",
    "    \n",
    "def compositeKeyGeneration(inverted_index):\n",
    "    '''sweeps the postings (vocabulary term, stripe word count) to make all possible pairs'''\n",
    "    word, postings = inverted_index\n",
    "    # taking advantage of symmetry, output only (a,b), but not (b,a)\n",
    "    for subset in itertools.combinations(sorted(postings), 2):\n",
    "        yield (str(subset), 1)\n",
    "    \n",
    "def similarityCalculations(line):\n",
    "    '''function that calculates all 4 metrics for each pair of terms'''\n",
    "    (term1, n1), (term2, n2) = ast.literal_eval(line[0])\n",
    "    \n",
    "    total = int(line[1])\n",
    "    \n",
    "    n1 = int(n1)\n",
    "    n2 = int(n2)\n",
    "    \n",
    "    cosine = total/float((n1*n2)**0.5)\n",
    "    jaccard = total/float(n1 + n2 - total)\n",
    "    overlap = total/float(min(n1, n2))\n",
    "    dice = total/float((n1+n2)/2)\n",
    "    \n",
    "    yield term1+\" - \"+term2, (cosine, jaccard, overlap, dice)\n",
    "\n",
    "def similarityAnalysis(stripesRDD, n):\n",
    "    \"\"\"\n",
    "    This function defines a Spark DAG to compute cosine, jaccard, \n",
    "    overlap and dice scores for each pair of words in the stripes\n",
    "    provided. \n",
    "    \n",
    "    Output: an RDD, a list of top n, a list of bottom n\n",
    "    \"\"\"\n",
    "    simScoresRDD, top_n, bottom_n = None, None, None\n",
    "    \n",
    "    ############### YOUR CODE HERE ################\n",
    "        \n",
    "    #spark job starts here\n",
    "    simScoresRDD = stripesRDD.flatMap(splitStripe) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(compositeKeyGeneration) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(similarityCalculations) \\\n",
    "             .cache()\n",
    "    \n",
    "    \n",
    "    top_n = simScoresRDD.takeOrdered(n, key=lambda x: -x[1][0])\n",
    "    bottom_n = simScoresRDD.takeOrdered(n, key=lambda x: x[1][0])\n",
    "\n",
    "    ############### (END) YOUR CODE ##############\n",
    "    return simScoresRDD, top_n, bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.5792744159698486 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "testResult, top_n, bottom_n = similarityAnalysis(testStripesRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "best - worst             |   1.0,     1.0,     1.0,   1.0\n",
      "foolishness - wisdom     |   1.0,     1.0,     1.0,   1.0\n",
      "times - wisdom           |0.5774,  0.3333,     1.0,   0.5\n",
      "age - best               |0.5774,  0.3333,     1.0,   0.5\n",
      "age - worst              |0.5774,  0.3333,     1.0,   0.5\n",
      "foolishness - times      |0.5774,  0.3333,     1.0,   0.5\n",
      "\n",
      "LEAST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "times - wisdom           |0.5774,  0.3333,     1.0,   0.5\n",
      "age - best               |0.5774,  0.3333,     1.0,   0.5\n",
      "age - worst              |0.5774,  0.3333,     1.0,   0.5\n",
      "foolishness - times      |0.5774,  0.3333,     1.0,   0.5\n",
      "best - worst             |   1.0,     1.0,     1.0,   1.0\n",
      "foolishness - wisdom     |   1.0,     1.0,     1.0,   1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"MOST SIMILAR:\")\n",
    "displayOutput(top_n)\n",
    "print(\"\")\n",
    "print(\"LEAST SIMILAR:\")\n",
    "displayOutput(bottom_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.1522917747497559 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "f1Result, top_n, bottom_n = similarityAnalysis(f1StripesRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "dans - memorable         |   1.0,     1.0,     1.0,   1.0\n",
      "names - novel            |   1.0,     1.0,     1.0,   1.0\n",
      "names - twelfth          |   1.0,     1.0,     1.0,   1.0\n",
      "novel - pulp             |   1.0,     1.0,     1.0,   1.0\n",
      "novel - twelfth          |   1.0,     1.0,     1.0,   1.0\n",
      "deadly - gas             |   1.0,     1.0,     1.0,   1.0\n",
      "deadly - hatred          |   1.0,     1.0,     1.0,   1.0\n",
      "deadly - installed       |   1.0,     1.0,     1.0,   1.0\n",
      "gas - hatred             |   1.0,     1.0,     1.0,   1.0\n",
      "hatred - poems           |   1.0,     1.0,     1.0,   1.0\n",
      "\n",
      "LEAST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "many - time              |0.0348,  0.0175,    0.04, 0.0345\n",
      "form - time              |0.0348,  0.0175,    0.04, 0.0345\n",
      "made - many              |0.0354,  0.0179,    0.04, 0.0351\n",
      "time - upon              |0.0355,  0.0179,  0.0417, 0.0351\n",
      "time - well              |0.0355,  0.0179,  0.0417, 0.0351\n",
      "time - way               |0.0355,  0.0179,  0.0417, 0.0351\n",
      "form - two               |0.0359,  0.0182,    0.04, 0.0357\n",
      "shall - two              |0.0359,  0.0182,    0.04, 0.0357\n",
      "made - way               |0.0361,  0.0182,  0.0417, 0.0357\n",
      "great - two              |0.0375,  0.0189,  0.0435, 0.037\n"
     ]
    }
   ],
   "source": [
    "print(\"MOST SIMILAR:\")\n",
    "displayOutput(top_n)\n",
    "print(\"\")\n",
    "print(\"LEAST SIMILAR:\")\n",
    "displayOutput(bottom_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1249.4658615589142 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "result, top_n, bottom_n = similarityAnalysis(stripesRDD, 20)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "first - time             |  0.89,  0.8012,  0.9149, 0.8897\n",
      "time - well              |0.8895,   0.801,   0.892, 0.8895\n",
      "great - time             | 0.875,  0.7757,   0.925, 0.8737\n",
      "part - well              | 0.874,  0.7755,  0.9018, 0.8735\n",
      "first - well             |0.8717,  0.7722,  0.8936, 0.8715\n",
      "part - time              |0.8715,  0.7715,  0.9018, 0.871\n",
      "time - upon              |0.8668,   0.763,  0.9152, 0.8656\n",
      "made - time              | 0.866,  0.7619,  0.9109, 0.8649\n",
      "made - well              |0.8601,  0.7531,  0.9022, 0.8592\n",
      "time - way               |0.8587,  0.7487,  0.9259, 0.8563\n",
      "great - well             |0.8526,  0.7412,  0.8988, 0.8514\n",
      "time - two               |0.8517,  0.7389,  0.9094, 0.8498\n",
      "first - great            |0.8497,  0.7381,  0.8738, 0.8493\n",
      "first - part             |0.8471,  0.7348,  0.8527, 0.8471\n",
      "great - upon             |0.8464,  0.7338,  0.8475, 0.8464\n",
      "upon - well              |0.8444,   0.729,   0.889, 0.8433\n",
      "new - time               |0.8426,   0.724,  0.9133, 0.8399\n",
      "first - two              |0.8411,  0.7249,  0.8737, 0.8405\n",
      "way - well               |0.8357,  0.7146,  0.8986, 0.8335\n",
      "time - us                |0.8357,  0.7105,  0.9318, 0.8308\n",
      "\n",
      "LEAST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "region - write           |0.0067,  0.0032,  0.0085, 0.0065\n",
      "relation - snow          |0.0067,  0.0026,  0.0141, 0.0052\n",
      "cardiac - took           |0.0074,  0.0023,  0.0217, 0.0045\n",
      "ever - tumor             |0.0076,   0.002,  0.0263, 0.004\n",
      "came - tumor             |0.0076,   0.002,  0.0263, 0.004\n",
      "let - therapy            |0.0076,   0.003,  0.0161, 0.0059\n",
      "related - stay           |0.0078,  0.0036,  0.0116, 0.0072\n",
      "factors - hear           |0.0078,  0.0039,  0.0094, 0.0077\n",
      "implications - round     |0.0078,  0.0033,  0.0145, 0.0066\n",
      "came - proteins          |0.0079,   0.002,  0.0286, 0.0041\n",
      "population - window      |0.0079,  0.0039,    0.01, 0.0077\n",
      "love - proportional      | 0.008,  0.0029,  0.0185, 0.0058\n",
      "got - multiple           | 0.008,  0.0034,  0.0149, 0.0067\n",
      "changes - fort           |0.0081,  0.0032,  0.0161, 0.0065\n",
      "layer - wife             |0.0081,  0.0038,  0.0119, 0.0075\n",
      "five - sympathy          |0.0081,  0.0034,  0.0149, 0.0068\n",
      "arrival - essential      |0.0081,   0.004,  0.0093, 0.008\n",
      "desert - function        |0.0081,  0.0031,  0.0175, 0.0062\n",
      "fundamental - stood      |0.0081,  0.0038,  0.0115, 0.0077\n",
      "patients - plain         |0.0081,   0.004,  0.0103, 0.0079\n"
     ]
    }
   ],
   "source": [
    "# part d - display the results (RUN THIS CELL AS IS)\n",
    "print(\"MOST SIMILAR:\")\n",
    "displayOutput(top_n)\n",
    "print(\"\")\n",
    "print(\"LEAST SIMILAR:\")\n",
    "displayOutput(bottom_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__  \n",
    "<table>\n",
    "<th>Most Similar</th>\n",
    "<th>Least Similar</th>\n",
    "<tr><td><pre>\n",
    "Pair           |Cosine, Jaccard, Overlap, Dice \n",
    "first - time   |  0.89,  0.8012,  0.9149, 0.8897\n",
    "time - well    |0.8895,   0.801,   0.892, 0.8895\n",
    "great - time   | 0.875,  0.7757,   0.925, 0.8737\n",
    "part - well    | 0.874,  0.7755,  0.9018, 0.8735\n",
    "first - well   |0.8717,  0.7722,  0.8936, 0.8715\n",
    "part - time    |0.8715,  0.7715,  0.9018, 0.871\n",
    "time - upon    |0.8668,   0.763,  0.9152, 0.8656\n",
    "made - time    | 0.866,  0.7619,  0.9109, 0.8649\n",
    "made - well    |0.8601,  0.7531,  0.9022, 0.8592\n",
    "time - way     |0.8587,  0.7487,  0.9259, 0.8563\n",
    "great - well   |0.8526,  0.7412,  0.8988, 0.8514\n",
    "time - two     |0.8517,  0.7389,  0.9094, 0.8498\n",
    "first - great  |0.8497,  0.7381,  0.8738, 0.8493\n",
    "first - part   |0.8471,  0.7348,  0.8527, 0.8471\n",
    "great - upon   |0.8464,  0.7338,  0.8475, 0.8464\n",
    "upon - well    |0.8444,   0.729,   0.889, 0.8433\n",
    "new - time     |0.8426,   0.724,  0.9133, 0.8399\n",
    "first - two    |0.8411,  0.7249,  0.8737, 0.8405\n",
    "way - well     |0.8357,  0.7146,  0.8986, 0.8335\n",
    "time - us      |0.8357,  0.7105,  0.9318, 0.8308\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "Pair                  |Cosine, Jaccard, Overlap, Dice \n",
    "region - write        |0.0067,  0.0032,  0.0085, 0.0065\n",
    "relation - snow       |0.0067,  0.0026,  0.0141, 0.0052\n",
    "cardiac - took        |0.0074,  0.0023,  0.0217, 0.0045\n",
    "ever - tumor          |0.0076,   0.002,  0.0263, 0.004\n",
    "came - tumor          |0.0076,   0.002,  0.0263, 0.004\n",
    "let - therapy         |0.0076,   0.003,  0.0161, 0.0059\n",
    "related - stay        |0.0078,  0.0036,  0.0116, 0.0072\n",
    "factors - hear        |0.0078,  0.0039,  0.0094, 0.0077\n",
    "implications - round  |0.0078,  0.0033,  0.0145, 0.0066\n",
    "came - proteins       |0.0079,   0.002,  0.0286, 0.0041\n",
    "population - window   |0.0079,  0.0039,    0.01, 0.0077\n",
    "love - proportional   | 0.008,  0.0029,  0.0185, 0.0058\n",
    "got - multiple        | 0.008,  0.0034,  0.0149, 0.0067\n",
    "changes - fort        |0.0081,  0.0032,  0.0161, 0.0065\n",
    "layer - wife          |0.0081,  0.0038,  0.0119, 0.0075\n",
    "five - sympathy       |0.0081,  0.0034,  0.0149, 0.0068\n",
    "arrival - essential   |0.0081,   0.004,  0.0093, 0.008\n",
    "desert - function     |0.0081,  0.0031,  0.0175, 0.0062\n",
    "fundamental - stood   |0.0081,  0.0038,  0.0115, 0.0077\n",
    "patients - plain      |0.0081,   0.004,  0.0103, 0.0079\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you've completed HW3! Please see the readme file for submission instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
