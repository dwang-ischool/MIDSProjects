{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 - Naive Bayes in Hadoop MR\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "In the live sessions for week 2 and week 3 you got some practice designing and debugging Hadoop Streaming jobs. In this homework we'll use Hadoop MapReduce to implement your first parallelized machine learning algorithm: Naive Bayes. As you develop your implementation you'll test it on a small dataset that matches the 'Chinese Example' in the _Manning, Raghavan and Shutze_ reading for Week 2. For the main task in this assignment you'll be working with a small subset of the Enron Spam/Ham Corpus. By the end of this assignment you should be able to:\n",
    "* __... describe__ the Naive Bayes algorithm including both training and inference.\n",
    "* __... perform__ EDA on a corpus using Hadoop MR.\n",
    "* __... implement__ parallelized Naive Bayes.\n",
    "* __... constrast__ partial, unordered and total order sort and their implementations in Hadoop Streaming.\n",
    "* __... explain__ how smoothing affects the bias and variance of a Multinomial Naive Bayes model.\n",
    "\n",
    "As always, your work will be graded both on the correctness of your output and on the clarity and design of your code. __Please refer to the `README` for homework submission instructions.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Before starting, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars (paths) - ADJUST AS NEEDED\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "HDFS_DIR = \"/user/root/HW2\"\n",
    "HOME_DIR = \"/media/notebooks/Assignments/HW2\" # FILL IN HERE eg. /media/notebooks/Assignments/HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "ENRON = \"data/enronemail_1h.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Hadoop MapReduce Key Takeaways.  \n",
    "\n",
    "This assignment will be the only one in which you use Hadoop Streaming to implement a distributed algorithm. The key reason we continue to teach Hadoop streaming is because of the way it forces the programmer to think carefully about what is happening under the hood when you parallelize a calculation. This question will briefly highlight some of the most important concepts that you need to understand about Hadoop Streaming and MapReduce before we move on to Spark next week.   \n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What \"programming paradigm\" is Hadoop MapReduce based on? What are the main ideas of this programming paradigm and how does MapReduce exemplify these ideas?\n",
    "\n",
    "* __b) short response:__ What is the Hadoop Shuffle? When does it happen? Why is it potentially costly? Describe one specific thing we can we do to mitigate the cost associated with this stage of our Hadoop Streaming jobs.\n",
    "\n",
    "* __c) short response:__ In Hadoop Streaming why do the input and output record format of a combiner script have to be the same? [__`HINT`__ _what level of combining does the framework guarantee? what is the relationship between the record format your mapper emits and the format your reducer expects to receive?_]\n",
    "\n",
    "* __d) short response:__ To what extent can you control the level of parallelization of your Hadoop Streaming jobs? Please be specific.\n",
    "\n",
    "* __e) short response:__ What change in the kind of computing resources available prompted the creation of parallel computation frameworks like Hadoop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Student Answers:\n",
    "\n",
    "> __a)__ Hadoop MapReduce is based on a programming paradigm that process large data sets with a parallel, distributed algorithm on a cluster. This programming paradigm consists of 2 stages - a first map stage which applies transformation or filtering to elements in a data set followed by a reduce stage in which task results are aggregated or folded based for similar elements.  The key idea exemplified by MapReduce is that of distributed parallel processing, in which the input data is partitioned and processed simultaneously within each partition during the map phase, and then results are aggregated in the reduce phase. \n",
    "\n",
    "> __b)__ The Hadoop shuffle is a process in which the data from the mapper is sorted and transfered to the reducer, where a shuffle is performed to ensure that key-value pairs with the same key end up in the same reducer. This process consists of 3 steps, the first being a partition step in which each output stream from the mapper is partitioned into separate files based on key, consisting of one file for each reducer. The next step consists of sorting records (key-value pairs) in the same partition, and the final step consists of the reducing within each partition where records of the same key are combined. On the reducer side, multiple files intended for each reducer are merge-sorted to perform aggregation by key more effectively. \n",
    "\n",
    "> This step is costly because it leads to performance degradation as we scale up with more data.  The sorting of the records, the communication of records between map and reduce steps, and the synchronization of the steps in the shuffle will all lead to increase in performance time as the data set becomes larger, and will quickly diminish the gains that come from the divide and conquer strategy of the MapReduce framework.  \n",
    "\n",
    "> The one specific thing we can do to mitigate cost is to perform local intermediate aggregation, where values for the same key are combined either on the mapper side or prior to transfer to the reducer.  When a combiner is used as an intermediate aggregation step, we decrease the number of records that are submitted to the shuffle and reduce the time it takes for communication, thereby decreasing the cost.  \n",
    "\n",
    "> __c)__ The input and output record format of a combiner has to be the same because the combiner may run several times during a streaming job or it may never run.  If the combiner is a separate script, meaning that it is not part of the mapper or reducer script, the combiner's input is the output from the mapper script, and the combiner's output is then the input for the reducer script.  In the event that a combiner does not run, the mapper output format and reducer input format must match exactly.  Therefore, a proper combiner's input format must match the mapper output format and the combiner's output much match the reducer's input format, which are the same.  \n",
    "\n",
    "> Another way to consider this is that if the input and output format of the combiner are different, then the combiner becomes responsible in transforming the data format between mapper and reducer.  However, if the combiner never runs, and the mapper output format does not match the reducer input format, the the job cannot finish. Therefore, the combiner input and output record format must match, so that they can match the mapper output and reducer input formats respectively.\n",
    "\n",
    "> __d)__ We can specify the level of parallelization in a Hadoop streaming job by specifying the number of mappers, the method of partitioning, and the number of reducers.  The number of mappers is used to specify the number of partitions in which the task is performed in parallel across the data. The number of mappers is driven by the DFS blocks in the input file so we can adjust the number of mappers by adjusting the dfs block size, however we do not have guaranteed control over the number of mappers. The method of partitioning can be custom and specifies how data is sorted across the reducers.  The number of reducers also specify a level of parallelization in which aggregation of values is performed for the same keys. The optimal level of parallelism is achieved by maps that take less than 1 minute to execute, which comes to a recommended number of 10-100 maps/node depending on the size of the task. The ideal number of reducers is the number closest to a multiple of the number of blocks and a task time of between 5-15 minutes, while creating the fewest files possible. \n",
    "\n",
    "> __e)__ The change in the middle of the 2000s that prompted the creation of parallel computing frameworks came from the semiconducter industry and the growth of large scale data problems.  The semiconductor industry ran out of opportunities to improve single-processor machines, but were able to find ways to produce existing processing resources at increasingly reduced costs. In order to tackle large data problems that cannot be processed by a single core, computer scientists and organizations shifted to using multiple cores for computing, where they can take advantage of investing in multiple cheaper processors rather than one very expensive high performing processor. The uses of multiple cores naturally lead to the need for divide and conquer processing strategies and therefore led to the advent of parallel computing frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: MapReduce Design Patterns.  \n",
    "\n",
    "In the last two live sessions and in your readings from Lin & Dyer you encountered a number of techniques for manipulating the logistics of a MapReduce implementation to ensure that the right information is available at the right time and location. In this question we'll review a few of the key techniques you learned.   \n",
    "\n",
    "### Q2 Tasks:\n",
    "\n",
    "* __a) short response:__ What are counters (in the context of Hadoop Streaming)? How are they useful? What kinds of counters does Hadoop provide for you? How do you create your own custom counter?\n",
    "\n",
    "* __b) short response:__ What are composite keys? How are they useful? How are they related to the idea of custom partitioning?\n",
    "\n",
    "* __c) short response:__ What is the order inversion pattern? What problem does it help solve? How do we implement it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Student Answers:\n",
    "\n",
    "> __a)__ Counters are objects, or variables that contain numbers, in the Hadoop streaming process that allow you to keep track of the progress in the map and reduce stages by counting the number of global events.  Hadoop provides for us counters such as the number and size of files read and written, the number of mapper and reducer jobs and the time they take, the number of records inputted and outputted by reducers, mappers and combiners, and the number and type of shuffle errors if they exist. These counters are a useful way for us to gauge the efficiency of our MapReduce streaming job and whether or not changes we make, such as writing a better combiner function, can improve the efficiency or not. \n",
    "\n",
    "> We can create our own custom counter by inserting this line - `sys.stderr.write(\"reporter:counter:counter-group,counter-name,#\\n\")` into the mapper, reducer, or combiner scripts with any accompanying logic.  The \"counter-name\" is where we enter a variable name for the custom counter, and the \"counter-group\" is where we enter a custom name for the the group under which the counter appears.  The # refers to the count that is incremented each time the function that encapsulates this line is ran.  \n",
    "\n",
    "> __b)__ A composite key is a key that consists of two or more fields, or the combination of two or more values that together form one unique key.  We actually use a composite key in #4 of this assignment, were the key is the combination of the the word and the value 0 or 1 to indicate spam or ham respectively.  Composite keys are useful in more complex operations where we need to group key-value pairs and then sort them in a meaningful way within these groups.  For instance, the composite key can consiste of a partition key and a sort key where the partition key is used to specify which partition or reducer key value pairs go to and the sort key is then used to determine how values are aggregated by the same key and outputed by the reducer. In question #4 for instance, the composite key is essential for us to perform custom partitioning using the spam/ham indicator part of the composite key, and we then use the sort key or word for the actual reducer function.   \n",
    "\n",
    "> __c)__ The order inversion pattern allows us to solve the problem of computing relative frequencies of words in the MapReduce framework without having to store the total dictionary of words in memory in order for us to access a total count. Following the example from the async, one variation of this type of problem is one in which we calculate the relative frequencies of co-occuring terms in a corpus.  In order to do this, we need to pass to the reducer a series of key-value pairs where the keys are composite keys with a primary key and a secondary key and the values are the intermediate counts for each pair of words.  The objective is then to calculate the frequency of the word that is the secondary key given the total count of the word that is the primary key.  If we attempt to store all primary key words in a dictionary to access their total counts, this could lead to a memory storage problem for larger corpuses.  Instead we can implement order inversion pattern.  First, specific to this example, we would want to specify custom partitioning such that key value pairs for the same primary key are passed to the same partition and therefore the same reducer.  Next to implement order inversion pattern, we create a key of the form (primary-key-word, \\*), such that the total count of the primary key is aggregated and comes out first in the reducer stream.  By using an \\*, we are essentially specifiying that we want the total count aggregation of all pairs with the same primary key regardless of secondary key, and that this value should appear before the individual totals of all other composite key pairs using the same primary key.  Because the reducer receives this marginal total first, it can then calculate the relative frequencies of each composite key, or word pair, using this total without having to store the primary keys to memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Understanding Total Order Sort\n",
    "\n",
    "The key challenge in distributed computing is to break a problem into a set of sub-problems that can be performed without communicating with each other. Ideally, we should be able to define an arbirtary number of splits and still get the right result, but that is not always possible. Parallelization becomes particularly challenging when we need to make comparisons between records, for example when sorting. Total Order Sort allows us to order large datasets in a way that enables efficient retrieval of results. Before beginning this assignment, make sure you have read and understand the [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb). You can skip the first two MRJob sections, but the rest of section III and all of section IV are **very** important (and apply to Hadoop Streaming) so make sure to read them closely. Feel free to read the Spark sections as well but you won't be responsible for that material until later in the course. To verify your understanding, answer the following questions.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ What is the difference between a partial sort, an unordered total sort, and a total order sort? From the programmer's perspective, what does total order sort allow us to do that we can't with unordered total? Why is this important with large datasets?\n",
    "\n",
    "* __b) short response:__ Which phase of a MapReduce job is leveraged to implement Total Order Sort? Which default behaviors must be changed. Why must they be changed?\n",
    "\n",
    "* __c) short response:__ Describe in words how to configure a Hadoop Streaming job for the custom sorting and partitioning that is required for Total Order Sort.  \n",
    "\n",
    "* __d) short response:__ Explain why we need to use an inverse hash code function.\n",
    "\n",
    "* __e) short response:__ Where does this function need to be located so that a Total Order Sort can be performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "\n",
    "> __a)__ Parital Sort is the default behavior of the Hadoop MapReduce output.  The default reducer output is mulple partition files where the key-value pairs are sorted within each patition based on key.  Unordered total sort is different is different in that there is a total ordering of keys across all partitions, where keys across different partitions are sorted relative to each other versus partial sort in which only keys in the same partition relative to each other.  However, in unordered total sort, because the paritions are not ordered themselves, they are not able to generate total order of keys.  Total Order sort is then different from unordered total sort in that the partitions themselves are stacked according to some order, such that it produces a total sorting of all of the keys.  \n",
    "\n",
    "> Total order sort allows us to have a complete ordering of key-value pairs within one map reduce job, which cannot be accomplished if we opt for unordered total sort.  Using unordered total sort, in order for us to achieve the same complete ordering of key-value pairs, we would need to include an additional post processing step to restack the partitions. This post processing step is increasingly more costly as the data sets becomes larger and the number of paritions becomes greater.\n",
    "\n",
    "> __b)__ The map phase and shuffle phase of the MapReduce job is leveraged to implement Total Order Sort. Within the map phase, the partition key and secondary sort key are defined such that mapper outputs contain both keys as separate fields. To accomplish Total Order Sort, three defalt behaviors must be changed.  The first is to define more than one field to be used as a composite key, since a partition key and sort key will both be needed.  This requires inserting a line to the Hadoop streaming job where we set the necessary number of fields used as the key via the stream.num.map.output.key.fields parameter to more than 1, so that the job knows to deviate from the default behavior of using only the first field from the mapper output as the key.  The second default behavior is how the mapper output records will be partitioned into reducers.  The default behavior of a Hadoop MapReduce only ensures that records from the mapper with the same key end up in the same reducer, but only partial sorting is acheived in the case of multiple reducer outputs.  This means that the default behavior cannot ensure that the first partition results are in order when compared to the second partition results, for instance.    However, Total order sort relies heavily on the correct paritioning of reducer records so that the output files for each reducer partition are naturally stacked to achieved ordering of all records across multiple partitions in order.  To acheive custom partitioning that deviates from the default behavior, we insert a line in the hadoop streaming job where we specificy the parameter mapreduce.partition.keypartitioner.options to be the field that is the parition key from the mapper. Lastly the sorting of records within each reducer partition output will need to be specfied as well.  The default behavior of record sorting in reducer partitions is lexicographically by the first field, assumed to be the key.  However, since Total Order Sort requires a composite key, we also need to specify the sort key for in-partition sorting via the parameter mapreduce.partition.keycomparator.options.  We can also specify in this parameter if we wish to have additional deviations in ordering, such as reverse-alphabetical or descending order of number.  \n",
    "\n",
    "> __c)__ Please refer to my answer in part (b) for additional details on the response here. Total Order Sort in Hadoop Streaming is accomplished by first transforming the data through the mapper function definition such that record outputs from the mapper consists of key-value pairs with a composite key.  The minimum composite key for the mapper output must consist of a parition key and a sorting key.  We then change the default behavior of the Hadoop shuffle through multiple lines inserted into our streaming job where we specify parameters for the composite key, the parition of mapper outputs to reducers and the order in which our output records are displayed within a reducer.  First by specifying a key consisting of more than one field, we allow the Hadoop MapReduce job to leverage both a partition key and a sort key instead of just defaulting to the first field from a mapper output record as the single key.  Next we change partitioner parameter within the job by specifying the exact field that is the partition key that we wish to use.  To ensure that records are sent to partitions in a ordered way, meaning that records sent to the first partition should appear in order before records that are sent to the second parition, and all subsequent partitions have records that are ordered from previous paritions, we use an inverse hash function to specify the parition key within the mapper.  If done correctly, this then ensures that outputs from the reducer appear in partitions in such a way that all of the records in each partition are ordered ahead of records in subsequent partitions.  Lastly, we insert a line into the job to specify the key comparator field, where we specify a secondary sort key and any deviations from the default sorting of records within each partition.  Since the default behavior sorts records in a partition by the first field, but the first field may not be the sort key, we use the key comparator field to specify how records within each partition should be sorted. We can also input sorting patterns that are in reverse order from the default behavior if necessary.  The end resut, if all steps are done correctly, should be a series of reducer outputs where records are sorted in order within each parition and across partitions, and the paritions themselves also appear to be in order.  \n",
    "\n",
    "> __d)__ By default, Hadoop has a HashParitioner class that it uses to compute the partition index for keys produced by mappers.  Depending on the number of reducers specified in the streaming job, the HashPartitioner class has a getPartition method that derives the partition index by hash modulo the number of reducers.  To achieve total order sort, we need to specify a partition key, such that mapper outputs are custom partitioned into each reducer.  The inverse hash function retrieves the getPartition resulting index, and generates a parition key for the mapper outputs. In the examples we have seen, a common form for the partition key is a letter, which can be used for default alphabetical sorting.  The inverse hash function essentially maps the parition index, an integer, to a letter.  Once mapper outputs have a proper partition key, and the custom partitioning by the partition key is specified within streaming job, mapper outputs are then sent to the reducer partitions in such a way that records sent to the first partition, index 0, are all ordered ahead of all records sent to the second parition, index 1, which are ahead of records sent to the third partition, index 2, and so on.  Once sorting is done of records within each partition, the reducer output files acheives total order sort, since all records within a partition are in order, and all records within each partition are ordered properly compared to subsequent partitions. \n",
    "\n",
    "> __e)__ The inverse hash function needs to be located in the mapper, so that all mapper output records have a parition key that is related to the parition index.  Only with a partition key that is outputted in mapper records are we then able to specify custom partitioning of mapper outputs to reducer partitions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "For the main task in this portion of the homework you will train a classifier to determine whether an email represents spam or not. You will train your Naive Bayes model on a 100 record subset of the Enron Spam/Ham corpus available in the HW2 data directory (__`HW2/data/enronemail_1h.txt`__).\n",
    "\n",
    "__Source:__   \n",
    "The original data included about 93,000 emails which were made public after the company's collapse. There have been a number raw and preprocessed versions of this corpus (including those available [here](http://www.aueb.gr/users/ion/data/enron-spam/index.html) and [here](http://www.aueb.gr/users/ion/publications.html)). The subset we will use is limited to emails from 6 Enron employees and a number of spam sources. It is part of [this data set](http://www.aueb.gr/users/ion/data/enron-spam/) which was created by researchers working on personlized Bayesian spam filters. Their original publication is [available here](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf). __`IMPORTANT!`__ _For this homework please limit your analysis to the 100 email subset which we provide. No need to download or run your analysis on any of the original datasets, those links are merely provided as context._\n",
    "\n",
    "__Preprocessing:__  \n",
    "For their work, Metsis et al. (the authors) appeared to have pre-processed the data, not only collapsing all text to lower-case, but additionally separating \"words\" by spaces, where \"words\" unfortunately include punctuation. As a concrete example, the sentence:  \n",
    ">  `Hey Jon, I hope you don't get lost out there this weekend!`  \n",
    "\n",
    "... would have been reduced by Metsis et al. to the form:  \n",
    "> `hey jon , i hope you don ' t get lost out there this weekend !` \n",
    "\n",
    "... so we have reverted the data back toward its original state, removing spaces so that our sample sentence would now look like:\n",
    "> `hey jon, i hope you don't get lost out there this weekend!`  \n",
    "\n",
    "Thus we have at least preserved contractions and other higher-order lexical forms. However, one must be aware that this reversion is not complete, and that some object (specifically web sites) will be ill-formatted, and that all text is still lower-cased.\n",
    "\n",
    "\n",
    "__Format:__   \n",
    "All messages are collated to a tab-delimited format:  \n",
    "\n",
    ">    `ID \\t SPAM \\t SUBJECT \\t CONTENT \\n`  \n",
    "\n",
    "where:  \n",
    ">    `ID = string; unique message identifier`  \n",
    "    `SPAM = binary; with 1 indicating a spam message`  \n",
    "    `SUBJECT = string; title of the message`  \n",
    "    `CONTENT = string; content of the message`   \n",
    "    \n",
    "Note that either of `SUBJECT` or `CONTENT` may be \"NA\", and that all tab (\\t) and newline (\\n) characters have been removed from both of the `SUBJECT` and `CONTENT` columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n",
      "0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n",
      "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n",
      "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n",
      "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n",
    "!head -n 5 {ENRON} | cut -c-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 data/enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "# see how many messages/lines are in the file \n",
    "#(this number may be off by 1 if the last line doesn't end with a newline)\n",
    "!wc -l {ENRON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {ENRON} {HDFS_DIR}/enron.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:  Enron Ham/Spam EDA.\n",
    "Before building our classifier, lets get aquainted with our data. In particular, we're interested in which words occur more in spam emails than in real emails. In this question you'll implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. You'll also learn about two new Hadoop streaming parameters that will allow you to control how the records output of your mappers are partitioned for reducing on separate nodes. \n",
    "\n",
    "__`IMPORTANT NOTE:`__ For this question and all subsequent items, you should include both the subject and the body of the email in your analysis (i.e. concatetate them to get the 'text' of the document).\n",
    "\n",
    "### Q4 Tasks:\n",
    "* __a) code:__ Complete the missing components of the code in __`EnronEDA/mapper.py`__ and __`EnronEDA/reducer.py`__ to create a Hadoop MapReduce job that counts how many times each word in the corpus occurs in an email for each class. Pay close attention to the data format specified in the docstrings of these scripts _-- there are a number of ways to accomplish this task, we've chosen this format to help illustrate a technique in `part e`_. Run the provided unit tests to confirm that your code works as expected then run the provided Hadoop streaming command to apply your analysis to the Enron data.\n",
    "\n",
    "\n",
    "* __b) code + short response:__ How many times does the word \"__assistance__\" occur in each class? (`HINT:` Use a `grep` command to read from the results file you generated in '`a`' and then report the answer in the space provided.)\n",
    "\n",
    "\n",
    "* __c) short response:__ Would it have been possible to add some sorting parameters to the Hadoop streaming command that would cause our `part a` results to be sorted by count? Briefly explain why or why not.\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Write a second Hadoop MapReduce job to sort the output of `part a` first by class and then by count. Run your job and save the results to a local file. Then describe in words how you would go about printing the top 10 words in each class given this sorted output. (`HINT 1:` _remember that you can simply pass the `part a` output directory to the input field of this job; `HINT 2:` since this task is just reodering the records from `part a` we don't need to write a mapper or reducer, just use `/bin/cat` for both_)\n",
    "\n",
    "\n",
    "* __ e) code:__ A more efficient alternative to '`grep`-ing' for the top 10 words in each class would be to use the Hadoop framework to separate records from each class into its own partition so that we can just read the top lines in each. Edit your job from ` part d` to specify 2 reduce tasks and to tell Hadoop to partition based on the second field (which indicates spam/ham in our data). Your code should maintain the secondary sort -- that is each partition should list words from most to least frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __b)__ Based on the results below, the word assistance appears twice for the ham class and 8 times in the spam class.  \n",
    "\n",
    "> __c)__ It is not possible to sort the part a results by count, or at least not by the total count of each word within each class.  The reason for this is because we can only specify sorting or records in the reducer output by values we get from the mapper.  The total count of each word is something that the mapper cannot output, as these are outputs of the reducer.  Since the total counts appear after the reducer has run, we will not be have to use them as a sort key for the reducer outputs.   \n",
    "\n",
    "> __d)__ In order to find the top occurring words within each class, we actually need a way to compare counts of words within each class.  This is most easily acheived by ensuring that mapper output records with the same class value are sent to the same reducer partition. In this sense, the class value of 0 or 1 can be used as the partition key for custom partitioning.  We can then print the top 10 occurring words within each class by sorting the reducer output records by within each class by count, which can be used as a sort key since we are using the output from part (a) as the input for the job in this part.  \n",
    "\n",
    "> Since reducer outputs are now partitioned by class, and sorted within each partition by count, we print the top 10 words for each class using the following way. For each partition, we can display the top 10 words by using a unix head function on the partition file, which will naturally display the top 10 rows of the partition file.  Since the records within the partition are already sorted by count, this display should output the top 10 most freqent words.  We can handle the \"for each partition\" part of this logic using a loop that loops through partition indexes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n",
    "!chmod a+x EnronEDA/mapper.py\n",
    "!chmod a+x EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\t1\t1\n",
      "body\t1\t1\n",
      "title\t0\t1\n",
      "body\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/mapper.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"d1\t1\ttitle\tbody\\nd2\t0\ttitle\tbody\" | EnronEDA/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\t0\t2\n",
      "one\t1\t1\n",
      "two\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/reducer.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"one\t1\t1\\none\t0\t1\\none\t0\t1\\ntwo\t0\t1\" | EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob3626335361420952488.jar tmpDir=null\n",
      "19/06/01 23:07:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:07:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:07:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/01 23:07:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/01 23:07:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0001\n",
      "19/06/01 23:07:26 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0001\n",
      "19/06/01 23:07:27 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0001/\n",
      "19/06/01 23:07:27 INFO mapreduce.Job: Running job: job_1559430286868_0001\n",
      "19/06/01 23:07:41 INFO mapreduce.Job: Job job_1559430286868_0001 running in uber mode : false\n",
      "19/06/01 23:07:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/01 23:08:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/01 23:08:28 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/01 23:08:30 INFO mapreduce.Job: Job job_1559430286868_0001 completed successfully\n",
      "19/06/01 23:08:30 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369010\n",
      "\t\tFILE: Number of bytes written=1335698\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217061\n",
      "\t\tHDFS: Number of bytes written=70551\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=47321\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=34818\n",
      "\t\tTotal time spent by all map tasks (ms)=47321\n",
      "\t\tTotal time spent by all reduce tasks (ms)=34818\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=47321\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=34818\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=48456704\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=35653632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=369022\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=369022\n",
      "\t\tReduce input records=31490\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=62980\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=452\n",
      "\t\tCPU time spent (ms)=13490\n",
      "\t\tPhysical memory (bytes) snapshot=942780416\n",
      "\t\tVirtual memory (bytes) snapshot=5469962240\n",
      "\t\tTotal committed heap usage (bytes)=763887616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216847\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70551\n",
      "19/06/01 23:08:30 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t0\t2\n",
      "assistance\t1\t8\n"
     ]
    }
   ],
   "source": [
    "# part b - write your grep command here\n",
    "!grep assistance EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/eda-sort-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part d/e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8209606843618682925.jar tmpDir=null\n",
      "19/06/01 23:10:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:10:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:10:04 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "19/06/01 23:10:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/01 23:10:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0002\n",
      "19/06/01 23:10:06 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0002\n",
      "19/06/01 23:10:06 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0002/\n",
      "19/06/01 23:10:06 INFO mapreduce.Job: Running job: job_1559430286868_0002\n",
      "19/06/01 23:10:23 INFO mapreduce.Job: Job job_1559430286868_0002 running in uber mode : false\n",
      "19/06/01 23:10:23 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/01 23:10:41 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/06/01 23:10:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/01 23:11:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/01 23:11:08 INFO mapreduce.Job: Job job_1559430286868_0002 completed successfully\n",
      "19/06/01 23:11:08 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=88743\n",
      "\t\tFILE: Number of bytes written=772740\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=70789\n",
      "\t\tHDFS: Number of bytes written=76611\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34279\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=42231\n",
      "\t\tTotal time spent by all map tasks (ms)=34279\n",
      "\t\tTotal time spent by all reduce tasks (ms)=42231\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34279\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=42231\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35101696\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=43244544\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6060\n",
      "\t\tMap output records=6060\n",
      "\t\tMap output bytes=76611\n",
      "\t\tMap output materialized bytes=88755\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6060\n",
      "\t\tReduce shuffle bytes=88755\n",
      "\t\tReduce input records=6060\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=12120\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=302\n",
      "\t\tCPU time spent (ms)=12960\n",
      "\t\tPhysical memory (bytes) snapshot=945467392\n",
      "\t\tVirtual memory (bytes) snapshot=5504081920\n",
      "\t\tTotal committed heap usage (bytes)=758644736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=70551\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76611\n",
      "19/06/01 23:11:08 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d/e - write your Hadoop streaming job here\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2n -k3,3nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\"  \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "the\t0\t549\t\n",
      "to\t0\t398\t\n",
      "ect\t0\t382\t\n",
      "and\t0\t278\t\n",
      "of\t0\t230\t\n",
      "hou\t0\t206\t\n",
      "a\t0\t196\t\n",
      "in\t0\t182\t\n",
      "for\t0\t170\t\n",
      "on\t0\t135\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "and\t1\t392\t\n",
      "your\t1\t357\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "in\t1\t236\t\n",
      "for\t1\t204\t\n",
      "com\t1\t153\t\n"
     ]
    }
   ],
   "source": [
    "# part e - view the top 10 records from each partition (RUN THIS CELL AS IS)\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__\n",
    "<table>\n",
    "<th>part-00000:</th>\n",
    "<th>part-00001:</th>\n",
    "<tr><td><pre>\n",
    "the\t0\t549\t\n",
    "to\t0\t398\t\n",
    "ect\t0\t382\t\n",
    "and\t0\t278\t\n",
    "of\t0\t230\t\n",
    "hou\t0\t206\t\n",
    "a\t0\t196\t\n",
    "in\t0\t182\t\n",
    "for\t0\t170\t\n",
    "on\t0\t135\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1\t698\t\n",
    "to\t1\t566\t\n",
    "and\t1\t392\t\n",
    "your\t1\t357\t\n",
    "a\t1\t347\t\n",
    "you\t1\t345\t\n",
    "of\t1\t336\t\n",
    "in\t1\t236\t\n",
    "for\t1\t204\t\n",
    "com\t1\t153\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Counters and Combiners.\n",
    "Tuning the number of mappers & reducers is helpful to optimize very large distributed computations. Doing so successfully requires a thorough understanding of the data size at each stage of the job. As you learned in the week3 live session, counters are an invaluable resource for understanding this kind of detail. In this question, we will take the EDA performed in Question 4 as an opportunity to illustrate some related concepts.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response:__ Read the Hadoop output from your job in Question 4a to report how many records are emitted by the mappers and how many records are received be the reducers. In the context of word counting what does this number represent practically?\n",
    "\n",
    "* __b) code:__ Note that we wrote the reducer in question 4a such that the input and output record format is identical. This makes it easy to use the same reducer script as a combiner. In the space provided below, write the Hadoop Streaming command to re-run your job from question 4a with this combining added.\n",
    "\n",
    "* __c) short response__: Report the number of records emitted by your mappers in part b and the number of records received by your reducers. Compare your results here to what you saw in part a. Explain.\n",
    "\n",
    "* __d) short response__: Describe a scenario where using a combiner would _NOT_ improve the efficiency of the shuffle stage. Explain. [__`BONUS:`__ how does increasing the number of mappers affect the usefulness of a combiner?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ Number emitted by mappers and received by reducers is the same number - 31490, Since my mapper outputs a 1 for every word in the corpus, this number represents the total number of words, defined by only consecutive letters, in the entire corpus of 100 emails, where the complete text consists of the body and subject of each email. \n",
    "\n",
    "> __c)__ Number emitted by mappers is still 31490, but the number received by reducers is 7648, which is also the number of records outputted by the combiners. This means that mapper performs the same way here as it did in 4(a), but fewer records are shuffled into the reducers because the combiner did run.  The combiner, since it has the same functionality that was defined in the reducer, has performed intermediate aggregation of key-value pairs with the same key emitted by the mappers. This then reduces the number of records that will need to be sent to the reducers, since intermediate aggregation has already combined some of the mapper outputs.  \n",
    "\n",
    "> __d)__ A scenario in which the combiner would not improve the efficiency of the shuffle stage is if the mapper was written in such a way that intermediate aggregation was already performed at the mapper phase and the records emitted by the mapper already consisted of key-value pairs with unique keys.  If the mapper phase already emits records with unique keys, then a combiner phase that performs additional intermediate aggregation would do very little to improve the efficiency of the shuffle stage, since the number of records outputted from the mappers and the number of records inputted to the reducers are the same, and a combiner will have done nothing to reduce the records inputed to reducers from mappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8562657381361214069.jar tmpDir=null\n",
      "19/06/01 23:15:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:15:12 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:15:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/01 23:15:14 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/01 23:15:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0003\n",
      "19/06/01 23:15:15 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0003\n",
      "19/06/01 23:15:15 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0003/\n",
      "19/06/01 23:15:15 INFO mapreduce.Job: Running job: job_1559430286868_0003\n",
      "19/06/01 23:15:28 INFO mapreduce.Job: Job job_1559430286868_0003 running in uber mode : false\n",
      "19/06/01 23:15:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/01 23:15:51 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/01 23:16:09 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/01 23:16:10 INFO mapreduce.Job: Job job_1559430286868_0003 completed successfully\n",
      "19/06/01 23:16:11 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=102833\n",
      "\t\tFILE: Number of bytes written=804772\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217061\n",
      "\t\tHDFS: Number of bytes written=70551\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=42323\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=30297\n",
      "\t\tTotal time spent by all map tasks (ms)=42323\n",
      "\t\tTotal time spent by all reduce tasks (ms)=30297\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=42323\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=30297\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=43338752\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=31024128\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=102845\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=31490\n",
      "\t\tCombine output records=7648\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=102845\n",
      "\t\tReduce input records=7648\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=15296\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=434\n",
      "\t\tCPU time spent (ms)=13130\n",
      "\t\tPhysical memory (bytes) snapshot=943853568\n",
      "\t\tVirtual memory (bytes) snapshot=5500428288\n",
      "\t\tTotal committed heap usage (bytes)=757071872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216847\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70551\n",
      "19/06/01 23:16:11 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your Hadoop streaming job here\n",
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -combiner reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Document Classification Task Overview.\n",
    "The week 2 assigned reading from Chapter 13 of _Introduction to Information Retrieval_ by Manning, Raghavan and Schutze provides a thorough introduction to the document classification task and the math behind Naive Bayes. In this question we'll use the example from Table 13.1 (reproduced below) to 'train' an unsmoothed Multinomial Naive Bayes model and classify a test document by hand.\n",
    "\n",
    "<table>\n",
    "<th>DocID</th>\n",
    "<th>Class</th>\n",
    "<th>Subject</th>\n",
    "<th>Body</th>\n",
    "<tr><td>Doc1</td><td>1</td><td></td><td>Chinese Beijing Chinese</td></tr>\n",
    "<tr><td>Doc2</td><td>1</td><td></td><td>Chinese Chinese Shanghai</td></tr>\n",
    "<tr><td>Doc3</td><td>1</td><td></td><td>Chinese Macao</td></tr>\n",
    "<tr><td>Doc4</td><td>0</td><td></td><td>Tokyo Japan Chinese</td></tr>\n",
    "</table>\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) short response:__ Equation 13.3 in Manning, Raghavan and Shutze shows how a Multinomial Naive Bayes model classifies a document. It predicts the class, $c$, for which the estimated conditional probability of the class given the document's contents,  $\\hat{P}(c|d)$, is greatest. In this equation what two pieces of information are required to calculate  $\\hat{P}(c|d)$? Your answer should include both mathematical notatation and verbal explanation.\n",
    "\n",
    "\n",
    "* __b) short response:__ The Enron data includes two classes of documents: `spam` and `ham` (they're actually labeled `1` and `0`). In plain English, explain what  $\\hat{P}(c)$ and   $\\hat{P}(t_{k} | c)$ mean in the context of this data. How will we would estimate these values from a training corpus? How many passes over the data would we need to make to retrieve this information for all classes and all words?\n",
    "\n",
    "\n",
    "* __c) hand calculations:__ Above we've reproduced the document classification example from the textbook (we added an empty subject field to mimic the Enron data format). Remember that the classes in this \"Chinese Example\" are `1` (about China) and `0` (not about China). Calculate the class priors and the conditional probabilities for an __unsmoothed__ Multinomial Naive Bayes model trained on this data. Show the calculations that lead to your result using markdown and $\\LaTeX$ in the space provided or by embedding an image of your hand written work. [`NOTE:` _Your results should NOT match those in the text -- they are training a model with +1 smoothing you are training a model without smoothing_]\n",
    "\n",
    "\n",
    "* __d) hand calculations:__ Use the model you trained to classify the following test document: `Chinese Chinese Chinese Tokyo Japan`. Show the calculations that lead to your result using markdown and   $\\LaTeX$ in the space provided or by embedding an image of your hand written work.\n",
    "\n",
    "\n",
    "* __e) short response:__ Compare the classification you get from this unsmoothed model in `d`/`e` to the results in the textbook's \"Example 1\" which reflects a model with Laplace plus 1 smoothing. How does smoothing affect our inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __a)__ The information that is needed to calculate $\\hat{P}(c|d)$ are $\\hat{P}(c)$ and $\\hat{P}(t_k|c)$ for all terms $t_k$ occuring in document c.  The probability $\\hat{P}(c)$  is called the prior and is denoted as $\\hat{P}(c) = \\frac{N_c}{N}$, or the relative frequency of documents of class c, calculated by occurrences of documents of class c divided by the total number of documents.  $\\hat{P}(t_k|c)$ is the conditional probability of term $t_k$ occurring in a document of class c.  The value $\\hat{P}(c|d)$ is then the product of $\\hat{P}(c)$ and all of the conditional probabilities $\\hat{P}(t_k|c)$  for each word in that document for which we are predicting the class.  This is then denoted by equation 13.2 - $\\hat{P}(c|d)\\propto\\hat{P}(c)\\Pi\\hat{P}(t_k|c)$\n",
    "\n",
    "> __b)__ $\\hat{P}(c)$ is the relative frequency of each class.  This is calculated by the number of emails of each class divided by the total number of emails, performed for each class.  In order to estimate these values from the training corpus, we will need to count the total number of lines of class 0 and class 1 to be able to calculate this value. $\\hat{P}(t_k|c)$ is the conditional probability of a word given a class.  For each word, $t_k$, this is calculated by the total count for the occurence of the composite key ($t_k$, c) divided by the count of total words in class c.  For example, if we would like to find the conditional probability of the word \"the\" given class spam, this is found by counting all occurences of the word \"the\" in spam emails and dividing by the total number of words in all spam emails.  \n",
    "\n",
    "> We can calculate both values by passing through the corpus data only once.  To count the number of emails within each class and calclate $\\hat{P}(c)$, we can leverage the structure of code in the mapper which reads each email as a line. In addition to emitting a count for each word, we can accumulate counts for emails by adding to 3 counters for total lines (or emails), spam emails and ham emails. To calculate $\\hat{P}(t_k|c)$, we need the counts of each unique ($t_k$, c) pair, which can be caculated by the reducer, provided we specify the composite key in the mapper.  However, to get the actual probability we need totals for words of class ham and spam, which we can also accumulate using 2 more counter variables in the mapper for loop.  When emitting key value pairs from the mapper, we should emit all words and their class and count, but we can also emit additional records that represent each of the 5 counters that are needed for probability calculations, so that they will reach the reducers.  However, to ensure the counters are accessbile by the reducers in advance of the probability calculations within the reducers, we will need to implement order inversion pattern, where keys for the counter values start with a special character so that they appear first.  If we need to do these probability calculations using multiple reducers, we also need to emit partition keys as part of our mapper outputs to ensure that all totals are sent to all reducers.  In summary, this method allows us to calculate these probabilities by passing through the data once. \n",
    "\n",
    "> __c)__ Show your calculations here using markdown & $\\LaTeX$ or embed them below!\n",
    "\n",
    "**Priors**\n",
    "\n",
    "$P(c=1)=3/4$\n",
    "\n",
    "$P(c=0)=1/4$\n",
    "\n",
    "**Conditional Probabilities**\n",
    "\n",
    "$P(chinese|1)=\\dfrac{2+2+1}{3+3+2}=5/8$\n",
    "\n",
    "$P(beijing|1)=\\dfrac{1}{3+3+2}=1/8$\n",
    "\n",
    "$P(shanghai|1)=\\dfrac{1}{3+3+2}=1/8$\n",
    "\n",
    "$P(macao|1)=\\dfrac{1}{3+3+2}=1/8$\n",
    "\n",
    "$P(tokyo|0)=P(japan|0)=P(chinese|0)=1/3$\n",
    "\n",
    "> __d)__ Show your calculations here using markdown & $\\LaTeX$ or embed them below!\n",
    "\n",
    "$P(0|d)=P(c=0)\\prod P(t_{k}|c)$\n",
    "\n",
    "$=P(c=0)\\cdot\\left[P(chinese|0)^{3}\\cdot P(tokyo|0)\\cdot P(japan|0)\\right]$\n",
    "\n",
    "$=\\dfrac{1}{4}\\cdot\\left[(\\dfrac{1}{3})^{3}\\cdot\\dfrac{1}{3}\\cdot\\dfrac{1}{3}\\right]=\\dfrac{1}{972}$\n",
    "\n",
    "$P(1|d)=P(c=1)\\prod P(t_{k}|c)$\n",
    "\n",
    "$=P(c=1)\\cdot\\left[P(chinese|1)\\cdot P(tokyo|1)\\cdot P(japan|1)\\right]$\n",
    "\n",
    "$=\\dfrac{1}{4}\\cdot\\left[(\\dfrac{1}{3})^{3}\\cdot0\\cdot0\\right]=0$\n",
    "\n",
    "> We then classify the document by choosing the class that resulted in the higher probability of ${P}(c|d)$, which is $\\boxed{class\\ 0}$  in this case.  \n",
    "\n",
    "> __e)__ With Laplace +1 smoothing, the probability of $P(1|d)$ is actually higer than  $P(0|d)$. In the unsomoothed model, since the words 'Japan' and 'Tokyo' never appear for class 1 in the training set, $P(1|d)$ was calculated as 0 given 0 conditional probability of these words for class 1. However, smoothing adds a small probability to events that are unseen in the training set, and thus with smoothing, $P(1|d)$ becomes non-zero.  However, the smoothing results affect our inference greatly, since in the presense of smoothing, we would have classfied this document as class 1 instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d/e - if you didn't write out your calcuations above, embed a picture of them here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"path-to-hand-calulations-image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Naive Bayes Inference.\n",
    "In the next two questions you'll write code to parallelize the Naive Bayes calculations that you performed above. We'll do this in two phases: one MapReduce job to perform training and a second MapReduce to perform inference. While in practice we'd need to train a model before we can use it to classify documents, for learning purposes we're going to develop our code in the opposite order. By first focusing on the pieces of information/format we need to perform the classification (inference) task you should find it easier to develop a solid implementation for training phase when you get to question 8 below. In both of these questions we'll continue to use the Chinese example corpus from the textbook to help us test our MapReduce code as we develop it. Below we've reproduced the corpus, test set and model in text format that matches the Enron data.\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ run the provided cells to create the example files and load them in to HDFS. Then take a closer look at __`NBmodel.txt`__. This text file represents a Naive Bayes model trained (with Laplace +1 smoothing) on the example corpus. What are the 'keys' and 'values' in this file? Which record means something slightly different than the rest? The value field of each record includes two numbers which will be helpful for debugging but which we don't actually need to perform inference -- what are they? [`HINT`: _This file represents the model from Example 13.1 in the textbook, if you're having trouble getting oriented try comparing our file to the numbers in that example._]\n",
    "\n",
    "\n",
    "* __b) short response:__ When performing Naive Bayes in practice instead of multiplying the probabilities (as in equation 13.3) we add their logs (as in equation 13.4). Why do we choose to work with log probabilities? If we had an unsmoothed model, what potential error could arise from this transformation?\n",
    "\n",
    "\n",
    "* __c) short response:__ Documents 6 and 8 in the test set include a word that did not appear in the training corpus (and as a result does not appear in the model). What should we do at inference time when we need a class conditional probability for this word?\n",
    "\n",
    "\n",
    "* __d) short response:__ The goal of our MapReduce job is to stream over the test set and classify each document by peforming the calculation from equation 13.4. To do this we'll load the model file (which contains the probabilities for equation 13.4) into memory on the nodes where we do our mapping. This is called an in-memory join. Does loading a model 'state' like this depart from the functional programming principles? Explain why or why not. From a scability perspective when would this kind of memory use be justified? when would it be unwise?\n",
    "\n",
    "\n",
    "* __e) code:__ Complete the code in __`NaiveBayes/classify_mapper.py`__. Read the docstring carefully to understand how this script should work and the format it should return. Run the provided unit tests to confirm that your script works as expected then write a Hadoop streaming job to classify the Chinese example test set. [`HINT 1:` _you shouldn't need a reducer for this one._ `HINT 2:` _Don't forget to add the model file to the_ `-files` _parameter in your Hadoop streaming job so that it gets shipped to the mapper nodes where it will be accessed by your script._]\n",
    "\n",
    "\n",
    "* __f) short response:__ In our test example and in the Enron data set we have fairly short documents. Since these fit fine in memory on a mapper node we didn't need a reducer and could just do all of our calculations in the mapper. However with much longer documents (eg. books) we might want a higher level of parallelization -- for example we might want to process parts of a document on different nodes. In this hypothetical scenario how would our algorithm design change? What could the mappers still do? What key-value structure would they emit? What would the reducers have to do as a last step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__ The keys are the words in the first column.  The value for each key is the list of 4 numbers which accompany each word.  The record that means something slightly different from the rest is the row for key = ClassPriors and the numbers that accompany it.  ClassPriors is not an actual word in the corpus, but it does contain the priors probability for each class, which is needed for inference. The values associated ClassPriors are the count of documents with class 0, the count of documents with class 1, the relative frequency of class 0 documents and the relative frequency of class 1 documents.  The value list has the following order - the total count of the word in class 0 documents, the total count of the word in class 1 documents, the class 0 conditional probability of the word, the class 1 conditional probability of the word.  The values not needed for inference are the first two - the total count of the word in each class.  \n",
    "\n",
    "> __b)__ When performing Naive Bayes calulations, we find the conditional probability of each class by multiplying the prior and conditional probabilities of each feature together.  The feature conditional probabilities might be very small the larger the data set, and thus the product caculation can lead to  floating point underflow.  By taking the log of the probability, we can still assess which class results in he highest conditional probability, without causing the underflow issue in our computation.  However, if there is no smoothing and we are taking into consideration a feature class pair that is does not exist in the training data, this would result in a term of log(P(feature|class))=log(0) which would lead to an undefined error.  \n",
    "\n",
    "> __c)__ While in Laplace smoothing we can assign non-zero probability to words that appear in the training set but do not appear for a particular class, we cannot do the same for words that never appear in the training set.  If we allowed smoothing to be applied for the word 'Trade' during our second MapReduce job for classification, we would be erroneous retraining our model in the process.  Instead, when performing classification, we should treat the words that never appear in the corpus as if they have have no effect on our posterior calculations.  In other words, when we are calculating log(P(class|document)), we should set the value for log(P('Trade'|class)) = 0 for both classes. \n",
    "\n",
    "> __d)__ No, I do not think that loading the model state into memory departs from functional programing principles.  Loading the model into memory does not violate the concept of statelessness that is essential to functional programming. While it is true that we are storing a \"state\", this state is actually not mutable during the entirety of the MapReduce job that performs the classification.  From a scalability perspective, the method of loading the model to memory is justified if the model is relative small compared to the amount of storage we have access to, and if we think that the feature set does not often change with future iterations of retraining.  It would be unwise to store the model in memory if the feature set is too large, say an extremely large corpus where the vocabulary is beyond our means to store, or if the feature set continues to grow with future iterations of re-training.\n",
    "\n",
    "> __e)__ Complete the coding portion of this question before answering 'f'.\n",
    "\n",
    "> __f)__ If we were processing the classfiication task on a very large corpus that is partitioned to multiple nodes, then we would not perform our predictions within the mapper, but rather the reducer.  Since the mapper outputs will be partitioned, the mapper should emit partial probabilities that can then be aggregated by key on the reducer side.  Key value pair structure that is emitted by mappers should then be  of the format - document id, class, log(P(0|d)), log(P(1|d)).  The reducer will then need to aggregate the partial probabilities log(P(0|d)), log(P(1|d)) by key, and in this case we can use the composite of document id and class as the key.  Lastly, the reducer will also calculate the prediction based on the larger of the two probabilities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to create the example corpus and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NBmodel.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NBmodel.txt\n",
    "beijing\t0.0,1.0,0.111111111111,0.142857142857\n",
    "chinese\t1.0,5.0,0.222222222222,0.428571428571\n",
    "tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "shanghai\t0.0,1.0,0.111111111111,0.142857142857\n",
    "ClassPriors\t1.0,3.0,0.25,0.75\n",
    "japan\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "macao\t0.0,1.0,0.111111111111,0.142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data files into HDFS\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTrain.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTest.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work for `part e` starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - do your work in NaiveBayes/classify_mapper.py first, then run this cell.\n",
    "!chmod a+x NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - unit test NaiveBayes/classify_mapper.py (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/chinese-output': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob5142590688070079451.jar tmpDir=null\n",
      "19/06/01 23:51:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:51:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/01 23:51:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/01 23:51:31 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/01 23:51:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0004\n",
      "19/06/01 23:51:32 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0004\n",
      "19/06/01 23:51:32 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0004/\n",
      "19/06/01 23:51:32 INFO mapreduce.Job: Running job: job_1559430286868_0004\n",
      "19/06/01 23:51:43 INFO mapreduce.Job: Job job_1559430286868_0004 running in uber mode : false\n",
      "19/06/01 23:51:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/01 23:52:00 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/06/01 23:52:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/01 23:52:11 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/01 23:52:12 INFO mapreduce.Job: Job job_1559430286868_0004 completed successfully\n",
      "19/06/01 23:52:12 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=192\n",
      "\t\tFILE: Number of bytes written=448793\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=403\n",
      "\t\tHDFS: Number of bytes written=178\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=28240\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8252\n",
      "\t\tTotal time spent by all map tasks (ms)=28240\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8252\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=28240\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8252\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=28917760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8450048\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=178\n",
      "\t\tMap output materialized bytes=198\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=198\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=127\n",
      "\t\tCPU time spent (ms)=3440\n",
      "\t\tPhysical memory (bytes) snapshot=758865920\n",
      "\t\tVirtual memory (bytes) snapshot=4118659072\n",
      "\t\tTotal committed heap usage (bytes)=625999872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=178\n",
      "19/06/01 23:52:12 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadooop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/chineseTest.txt \\\n",
    "  -output {HDFS_DIR}/chinese-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - retrieve test set results from HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-000* > NaiveBayes/chineseResults.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - take a look (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseResults.txt | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th> Expected output for the test set:</th>\n",
    "<tr align=Left><td><pre>\n",
    "d5\t1\t-8.90668134\t-8.10769031\t1\n",
    "d6\t1\t-5.78074351\t-4.17950237\t1\n",
    "d7\t0\t-6.59167373\t-7.51170688\t0\n",
    "d8\t0\t-4.39444915\t-5.56579673\t0\n",
    "</pre></td><tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Naive Bayes Training.\n",
    "In Question 7 we used a model that we had trained by hand. Next we'll develop the code to do that same training in parallel, making it suitable for use with larger corpora (like the Enron emails). The end result of the MapReduce job you write in this question should be a model text file that looks just like the example (`NBmodel.txt`) that we created by hand above.\n",
    "\n",
    "To refresh your memory about the training process take a look at  `6a` and `6b` where you described the pieces of information you'll need to collect in order to encode a Multinomial Naive Bayes model. We now want to retrieve those pieces of information while streaming over a corpus. The bulk of the task will be very similar to the word counting excercises you've already done but you may want to consider a slightly different key-value record structure to efficiently tally counts for each class. \n",
    "\n",
    "The most challenging (interesting?) design question will be how to retrieve the totals (# of documents and # of words in documents for each class). Of course, counting these numbers is easy. The hard part is the timing: you'll need to make sure you have the counts totalled up _before_ you start estimating the class conditional probabilities for each word. It would be best (i.e. most scalable) if we could find a way to do this tallying without storing the whole vocabulary in memory... Use an appropriate MapReduce design pattern to implement this efficiently! \n",
    "\n",
    "__IMPORTANT NOTE:__ For full credit on this question, your code must work with multiple reducers. [`HINT:` _You will need to implement Total Order Sort_ - [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb)]\n",
    "\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) make a plan:__  Fill in the docstrings for __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ to appropriately reflect the format that each script will input/output. [`HINT:` _the input files_ (`enronemail_1h.txt` & `chineseTrain.txt`) _have a prespecified format and your output file should match_ `NBmodel.txt` _so you really only have to decide on an internal format for Hadoop_].\n",
    "\n",
    "\n",
    "* __b) implement it:__ Complete the code in __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ so that together they train a Multinomial Naive Bayes model __with no smoothing__. Make sure your end result is formatted correctly (see note above). Test your scripts independently and together (using `chineseTrain.txt` or test input of your own devising). When you are satisfied with your Python code design and run a Hadoop streaming command to run your job in parallel on the __chineseTrain.txt__. Confirm that your trained model matches your hand calculations from Question 5.\n",
    "\n",
    "\n",
    "* __c) short response:__ We saw in Question 6 that adding Laplace +1 smoothing makes our classifications less sensitve to rare words. However implementing this technique requires access to one additional piece of information that we had not previously used in our Naive Bayes training. What is that extra piece of information? [`HINT:` see equation 13.7 in Manning, Raghavan and Schutze].\n",
    "\n",
    "\n",
    "* __d) short response:__ There are three approaches that we could take to handle the extra piece of information you identified in `c`: 1) we could hard code it into our reducer (_where would we get it in the first place?_). Or 2) we could compute it inside the reducer which would require storing some information in memory (_what information?_). Or 3) we could compute it in the reducer without storing any bulky information in memory but then we'd need some postprocessing or a second MapReduce job to complete the calculation (_why?_). Breifly explain what is non-ideal about each of these options. BONUS: which of these options is incompatible with using multiple reducers?\n",
    "\n",
    "\n",
    "* __e) code + short response:__ Choose one of the 3 options above. State your choice & reasoning in the space below then use that strategy to complete the code in __`NaiveBayes/train_reducer_smooth.py`__. Test this alternate reducer then write and run a Hadoop streaming job to train an MNB model with smoothing on the Chinese example. Your results should match the model that we provided for you above (and the calculations in the textbook example). [`HINT:` _don't start from scratch with this one -- you can just copy over your reducer code from part `b` and make the needed modifications_]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "> __ c)__ Laplace +1 smoothing requires that when we calculate the probability of each feature conditioned on class, $\\hat{P}(t_k|c)$, we add 1 to the numerator (occurences of the word in class c) and we add the number unique words in the corpus to the denominator (the total number of words in the class).  The information that we are currently missing in our Naive Bayes training is the number of unique words in the corpus.  \n",
    "\n",
    "> __ d)__ In option 1, the hard coding method is non-ideal if the training data changes such that the dictionary of unique words changes.  We can obtain the unique word counts externally by counting the number of lines from the output file of the unsmoothed NB training job, where we exclude the ClassPriors lines. If we choose to hard code unique word counts, then we need to recalculate unique word totals each time re-trainining occurs with test data that could lead to dictionary changes.  This prevents us from frequent retraining, since we have an additional steps of calculating the unique words and updating the code, which is very unscalable.  \n",
    "\n",
    ">In option 2, we can get the set of unique words by commiting information to memory, such that the information is a dictionary that stores each unique word from the corpus. Comitting the dictionary of unique words to memory is not ideal in the case that model retraining with test data causes the dictionary to grow. If the dictionary grows too large as the training data scales up, then the processor will eventually crash due to lack of memory.  \n",
    "\n",
    "> Option 3 where we use postprocessing or a second MapReduce job where we input the output from the first job could be non-ideal in that it becomes increasingly more costly with larger vocabulary size.  We would need a second job or post processing because the only way to count the unique words for each class without committing to memory is calculate smoothed probabilities and write them to record/files instantenously. The output from the initial job that generates an unsmoothed model will be very large if the corpus is large, and while we are not committing the dictionary to memory, a poorly written second MapReduce job could be costly in processing power, especially in its shuffle phase. \n",
    "\n",
    "> __ e)__ The option I have chosen to implement is the second MapReduce job.  However, in order to do so, I will also be writing a mapper so that the job can work with multiple reducers.  The input for the second map reduce job will the be output file from the first job containing the unsmoothed model.  In the mapper, I will loop through all rows to count the number of unique words.  I will then emit records for the total unique words and ensure that all totals are passed to all reducer paritions using a custom partition key.  The mapper will also emit each input row as an output along with one row for the Class Priors so that the word counts can be used by the reducer to calculate the new probabilities.  Order inversion pattern will again be implemented so that the unique counts appear first in reducer paritions.  The reducer will then use these unique counts to calculate Laplace +1 smoothing probabilities for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== MAPPER DOCSTRING ============\n",
      "Each line of document is tokenized into words, where words are continuous strings \n",
      "Order Inversion Pattern will be implemented to emit additional lines that count:\n",
      "  - total number of lines or documents\n",
      "  - total number of lines by class\n",
      "  - total number of words by class\n",
      "In addition, a custom partition key is added to ensure job works for multiple reducers    \n",
      "=========== REDUCER DOCSTRING ============\n",
      "Using assumption from implementing order inversion pattern, \n",
      "\tthe necessary total counts for frequency calculations are assumed to be at the top of each input file\n",
      "5 Total counts outputed from he mapper and sent to every reducer are first extracted and stored\n",
      "Then spam/ham counts and frequencies are calculated for subsequent words\n",
      "An additional line is added to print the final word\n",
      "Another additional line is added to print the ClassPrior row for class counts and prior probabilities    \n"
     ]
    }
   ],
   "source": [
    "# part a - do your work in train_mapper.py and train_reducer.py then RUN THIS CELL AS IS\n",
    "!chmod a+x NaiveBayes/train_mapper.py\n",
    "!chmod a+x NaiveBayes/train_reducer.py\n",
    "!echo \"=========== MAPPER DOCSTRING ============\"\n",
    "!head -n 9 NaiveBayes/train_mapper.py | tail -n 6\n",
    "!echo \"=========== REDUCER DOCSTRING ============\"\n",
    "!head -n 9 NaiveBayes/train_reducer.py | tail -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part b starts here`:__ MNB _without_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!totaldocs\tA\t0\t4\n",
      "!totalhamdocs\tA\t0\t1\n",
      "!totalhamwords\tA\t1\t3\n",
      "!totalspamdocs\tA\t0\t3\n",
      "!totalspamwords\tA\t1\t8\n",
      "beijing\tA\t1\t1\n",
      "chinese\tA\t0\t1\n",
      "chinese\tA\t1\t1\n",
      "chinese\tA\t1\t1\n",
      "chinese\tA\t1\t1\n",
      "chinese\tA\t1\t1\n",
      "chinese\tA\t1\t1\n",
      "!totaldocs\tB\t0\t4\n",
      "!totalhamdocs\tB\t0\t1\n",
      "!totalhamwords\tB\t1\t3\n",
      "!totalspamdocs\tB\t0\t3\n",
      "!totalspamwords\tB\t1\t8\n",
      "japan\tB\t0\t1\n",
      "macao\tB\t1\t1\n",
      "!totaldocs\tC\t0\t4\n",
      "!totalhamdocs\tC\t0\t1\n",
      "!totalhamwords\tC\t1\t3\n",
      "!totalspamdocs\tC\t0\t3\n",
      "!totalspamwords\tC\t1\t8\n",
      "shanghai\tC\t1\t1\n",
      "tokyo\tC\t0\t1\n",
      "!totaldocs\tD\t0\t4\n",
      "!totalhamdocs\tD\t0\t1\n",
      "!totalhamwords\tD\t1\t3\n",
      "!totalspamdocs\tD\t0\t3\n",
      "!totalspamwords\tD\t1\t8\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your mapper here\n",
    "!python NaiveBayes/train_mapper.py < NaiveBayes/chineseTrain.txt | sort -k2,2 -k1,1 > NaiveBayes/unitTestMapper.txt\n",
    "!cat NaiveBayes/unitTestMapper.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing\t0.0,1.0,0.0,0.125\n",
      "chinese\t1.0,5.0,0.3333333333333333,0.625\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your reducer here\n",
    "\n",
    "#please note, my unit test only runs for one custom, since I could not simulate multiple reducers\n",
    "#the keys used for total counts are repeated in the mapper out so that partial counts can be sent to each reducer and aggregated\n",
    "#however, since I cannot easily write paritions for multiple reducers in my unit test, I only run unit test for one parition\n",
    "!head -n 12 NaiveBayes/unitTestMapper.txt | NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beijing\t0.0,1.0,0.0,0.125\n",
      "chinese\t1.0,5.0,0.3333333333333333,0.625\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here\n",
    "!python NaiveBayes/train_mapper.py < NaiveBayes/chineseTrain.txt | sort -k2,2 -k1,1 | head -n 12 | NaiveBayes/train_reducer.py > NaiveBayes/unsmoothNBmodel.txt\n",
    "!cat NaiveBayes/unsmoothNBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/unsmoothNB-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear (and name) an output directory in HDFS for your unsmoothed chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/unsmoothNB-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob4737498598478850876.jar tmpDir=null\n",
      "19/06/02 04:45:26 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 04:45:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 04:45:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/02 04:45:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/02 04:45:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0024\n",
      "19/06/02 04:45:30 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0024\n",
      "19/06/02 04:45:30 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0024/\n",
      "19/06/02 04:45:30 INFO mapreduce.Job: Running job: job_1559430286868_0024\n",
      "19/06/02 04:45:43 INFO mapreduce.Job: Job job_1559430286868_0024 running in uber mode : false\n",
      "19/06/02 04:45:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 04:45:59 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/06/02 04:46:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 04:46:24 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "19/06/02 04:46:28 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "19/06/02 04:46:29 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "19/06/02 04:46:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 04:46:31 INFO mapreduce.Job: Job job_1559430286868_0024 completed successfully\n",
      "19/06/02 04:46:31 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1083\n",
      "\t\tFILE: Number of bytes written=904868\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=387\n",
      "\t\tHDFS: Number of bytes written=312\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=25513\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=98609\n",
      "\t\tTotal time spent by all map tasks (ms)=25513\n",
      "\t\tTotal time spent by all reduce tasks (ms)=98609\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=25513\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=98609\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=26125312\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=100975616\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=51\n",
      "\t\tMap output bytes=957\n",
      "\t\tMap output materialized bytes=1107\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=26\n",
      "\t\tReduce shuffle bytes=1107\n",
      "\t\tReduce input records=51\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=102\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=535\n",
      "\t\tCPU time spent (ms)=11450\n",
      "\t\tPhysical memory (bytes) snapshot=1358725120\n",
      "\t\tVirtual memory (bytes) snapshot=8269000704\n",
      "\t\tTotal committed heap usage (bytes)=950009856\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=159\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=312\n",
      "19/06/02 04:46:31 INFO streaming.StreamJob: Output directory: /user/root/HW2/unsmoothNB-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2 -k1,1\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\"  \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/unsmoothNB-output \\\n",
    "  -numReduceTasks 4 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "beijing\t0.0,1.0,0.0,0.125\n",
      "chinese\t1.0,5.0,0.3333333333333333,0.625\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "\n",
      "===== part-00002=====\n",
      "\n",
      "japan\t1.0,0.0,0.3333333333333333,0.0\n",
      "macao\t0.0,1.0,0.0,0.125\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "\n",
      "===== part-00003=====\n",
      "\n",
      "shanghai\t0.0,1.0,0.0,0.125\n",
      "tokyo\t1.0,0.0,0.3333333333333333,0.0\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "#print all reducer outputs for a final check:\n",
    "\n",
    "for idx in range(4):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/unsmoothNB-output/part-0000{idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/unsmoothNB-output/part-000* > NaiveBayes/unsmoothNBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "beijing\t0.0,1.0,0.0,0.125\n",
      "chinese\t1.0,5.0,0.3333333333333333,0.625\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "japan\t1.0,0.0,0.3333333333333333,0.0\n",
      "macao\t0.0,1.0,0.0,0.125\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "shanghai\t0.0,1.0,0.0,0.125\n",
      "tokyo\t1.0,0.0,0.3333333333333333,0.0\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n"
     ]
    }
   ],
   "source": [
    "# part b - print your model so that we can confirm that it matches expected results\n",
    "!cat NaiveBayes/unsmoothNBmodel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part e starts here`:__ MNB _with_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!unique\tA\t3.0,8.0,6.0,0\n",
      "ClassPriors\tA\t1.0,3.0,0.25,0.75\n",
      "beijing\tA\t0.0,1.0,0.0,0.125\n",
      "chinese\tA\t1.0,5.0,0.3333333333333333,0.625\n",
      "!unique\tB\t3.0,8.0,6.0,0\n",
      "japan\tB\t1.0,0.0,0.3333333333333333,0.0\n",
      "macao\tB\t0.0,1.0,0.0,0.125\n",
      "!unique\tC\t3.0,8.0,6.0,0\n",
      "shanghai\tC\t0.0,1.0,0.0,0.125\n",
      "tokyo\tC\t1.0,0.0,0.3333333333333333,0.0\n",
      "!unique\tD\t3.0,8.0,6.0,0\n"
     ]
    }
   ],
   "source": [
    "# part e - unit test for my new mapper\n",
    "!python NaiveBayes/train_mapper_smooth.py < NaiveBayes/unsmoothNBmodel.txt | sort -k2,2 -k1,1 > NaiveBayes/unitTestMapperSmooth.txt\n",
    "!cat NaiveBayes/unitTestMapperSmooth.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x NaiveBayes/train_mapper_smooth.py\n",
    "!chmod a+x NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "beijing\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "chinese\t1.0,5.0,0.2222222222222222,0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# part e - write a unit test for your NEW reducer here\n",
    "\n",
    "#please note, my unit test only runs for one custom, since I could not simulate multiple reducers\n",
    "#the keys used for total counts are repeated in the mapper out so that partial counts can be sent to each reducer and aggregated\n",
    "#however, since I cannot easily write paritions for multiple reducers in my unit test, I only run unit test for one parition\n",
    "!head -n 4 NaiveBayes/unitTestMapperSmooth.txt | NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "beijing\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "chinese\t1.0,5.0,0.2222222222222222,0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# part e - write a systems test for your mapper + reducer together here\n",
    "!python NaiveBayes/train_mapper_smooth.py < NaiveBayes/unsmoothNBmodel.txt | sort -k2,2 -k1,1 | head -n 4 | NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smoothNB-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear (and name) an output directory in HDFS for your SMOOTHED chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smoothNB-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8689789598324640198.jar tmpDir=null\n",
      "19/06/02 04:28:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 04:28:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 04:29:01 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "19/06/02 04:29:01 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/06/02 04:29:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0020\n",
      "19/06/02 04:29:02 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0020\n",
      "19/06/02 04:29:02 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0020/\n",
      "19/06/02 04:29:02 INFO mapreduce.Job: Running job: job_1559430286868_0020\n",
      "19/06/02 04:29:19 INFO mapreduce.Job: Job job_1559430286868_0020 running in uber mode : false\n",
      "19/06/02 04:29:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 04:29:46 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/06/02 04:29:48 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/06/02 04:29:50 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/06/02 04:29:51 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 04:30:15 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "19/06/02 04:30:17 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "19/06/02 04:30:18 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 04:30:20 INFO mapreduce.Job: Job job_1559430286868_0020 completed successfully\n",
      "19/06/02 04:30:20 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=792\n",
      "\t\tFILE: Number of bytes written=1205964\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=816\n",
      "\t\tHDFS: Number of bytes written=445\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=101603\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=107353\n",
      "\t\tTotal time spent by all map tasks (ms)=101603\n",
      "\t\tTotal time spent by all reduce tasks (ms)=107353\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=101603\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=107353\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=104041472\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=109929472\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=716\n",
      "\t\tMap output materialized bytes=864\n",
      "\t\tInput split bytes=504\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce shuffle bytes=864\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=52\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=886\n",
      "\t\tCPU time spent (ms)=14160\n",
      "\t\tPhysical memory (bytes) snapshot=1856471040\n",
      "\t\tVirtual memory (bytes) snapshot=11025141760\n",
      "\t\tTotal committed heap usage (bytes)=1340604416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=312\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=445\n",
      "19/06/02 04:30:20 INFO streaming.StreamJob: Output directory: /user/root/HW2/smoothNB-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your hadoop streaming job\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2 -k1,1\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\"  \\\n",
    "  -files NaiveBayes/train_mapper_smooth.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper_smooth.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/unsmoothNB-output \\\n",
    "  -output {HDFS_DIR}/smoothNB-output \\\n",
    "  -numReduceTasks 4 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "beijing\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "chinese\t1.0,5.0,0.2222222222222222,0.42857142857142855\n",
      "\n",
      "===== part-00002=====\n",
      "\n",
      "japan\t1.0,0.0,0.2222222222222222,0.07142857142857142\n",
      "macao\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "\n",
      "===== part-00003=====\n",
      "\n",
      "shanghai\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "tokyo\t1.0,0.0,0.2222222222222222,0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "#print all reducer outputs for a final check:\n",
    "\n",
    "for idx in range(4):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/smoothNB-output/part-0000{idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "ClassPriors\t1.0,3.0,0.25,0.75\n",
      "beijing\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "chinese\t1.0,5.0,0.2222222222222222,0.42857142857142855\n",
      "japan\t1.0,0.0,0.2222222222222222,0.07142857142857142\n",
      "macao\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "shanghai\t0.0,1.0,0.1111111111111111,0.14285714285714285\n",
      "tokyo\t1.0,0.0,0.2222222222222222,0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "# part e - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/smoothNB-output/part-000* > NaiveBayes/smoothNBmodel.txt\n",
    "!cat NaiveBayes/smoothNBmodel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Enron Ham/Spam NB Classifier & Results.\n",
    "\n",
    "Fantastic work. We're finally ready to perform Spam Classification on the Enron Corpus. In this question you'll run the analysis you've developed, report its performance, and draw some conclusions.\n",
    "\n",
    "### Q9 Tasks:\n",
    "* __a) train/test split:__ Run the provided code to split our Enron file into a training set and testing set then load them into HDFS. \n",
    "\n",
    "[`NOTE:` _If you hard coded the vocab size in question 8d make sure you re calculate the vocab size for just the training set!_]\n",
    "\n",
    "* __b) train 2 models:__ Write Hadoop Streaming jobs to train MNB Models on the training set with and without smoothing. Save your models to local files at __`NaiveBayes/Unsmoothed/NBmodel.txt`__ and __`NaiveBayes/Smoothed/NBmodel.txt`__. [`NOTE:` _This naming is important because we wrote our classification task so that it expects a file of that name... if this inelegance frustrates you there is an alternative that would involve a few adjustments to your code [read more about it here](http://www.tnoda.com/blog/2013-11-23)._] Finally run the checks that we provide to confirm that your results are correct.\n",
    "\n",
    "\n",
    "* __c) code:__ Recall that we designed our classification job with just a mapper. An efficient way to report the performance of our models would be to simply add a reducer phase to this job and compute precision and recall right there. Complete the code in __`NaiveBayes/evaluation_reducer.py`__ and then write Hadoop jobs to evaluate your two models on the test set. Report their performance side by side. [`NOTE:` if you need a refresher on precision, recall and F1-score [Wikipedia](https://en.wikipedia.org/wiki/F1_score) is a good resource.]\n",
    "\n",
    "\n",
    "* __d) short response:__ Compare the performance of your two models. What do you notice about the unsmoothed model's predictions? Can you guess why this is happening? Which evaluation measure do you think is most relevant in our use case? [`NOTE:` _Feel free to answer using your common sense but if you want more information on evaluating the classification task checkout_ [this blogpost](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\n",
    ") or [this paper](http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf\n",
    ")]\n",
    "\n",
    "\n",
    "* __e) code + short response:__ Let's look at the top ten words with the highest conditional probability in `Spam` and in `Ham`. We'll do this by writing a Hadoop job that sorts the model file (`NaiveBayes/Smoothed/NBmodel.py`). Normally we'd have to run two jobs -- one that sorts on $P(word|ham)$ and another that sorts on $P(word|spam)$. However if we slighly modify the data format in the model file then we can get the top words in each class with just one job. We've written a mapper that will do just this for you. Read through __`NaiveBayes/model_sort_mapper.py`__ and then briefly explain how this mapper will allow us to partition and sort our model file. Write a Hadoop job that uses our mapper and `/bin/cat` for a reducer to partition and sort. Print out the top 10 words in each class (where 'top' == highest conditional probability).[`HINT:` _this should remind you a lot of what we did in Question 6._]\n",
    "\n",
    "\n",
    "* __f) short response:__ What do you notice about the 'top words' we printed in `e`? How would increasing the smoothing parameter 'k' affect the probabilities for the top words that you identified for 'e'. How would they affect the probabilities of words that occur much more in one class than another? In summary, how does the smoothing parameter 'k' affect the bias and the variance of our model. [`NOTE:` _you do not need to code anything for this task, but if you are struggling with it you could try changing 'k' and see what happens to the test set. We don't recommend doing this exploration with the Enron data because it will be harder to see the impact with such a big vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "> __d)__ The unsmoothed model has lower accuracy, lower recall, and lower F-score than the smoothed model.  The smoothed model has slightly higher accuracy.  Most noticeably, the true positive count is very high for the smoothed model compared to the unsmoothed model and the false negative count is very high for the unsmoothed model compared to the smoothed model.  This may be happening because some of the train set and test set are both small and some of the emails are very short.  Without smoothing, the NB models punishes heavily for test emails that have words that may not appear in train emails of the same class.  As a result, the model has a hard time predicting documents with words that appear for a class in the test data but does not appear for the same class in the training data.  Since a lot of words are natural language fillers, (the, to, of, etc....), smoothing allows us to assign small probabilities to words that appear in the test set for a class but not in the training set for the same class.  This then prevents the possibility for test emails that might contain addition fillers or missing fillers compared to train emails of the same class to be predicted to have the wrong class.  The evaluation score that is most relevant is the F1-Score.  \n",
    "\n",
    "> __e)__ The NaiveBayes/model_sort_mapper.py intakes rows from the NBModel file, compares the spam and ham conditional probabilities, and outputs new key value pairs, where the keys but the values include the original values in the input plus 2 more fields, one for the maximum probability and one for the prediced class associated with the maximum probability.  To find the top 10 words for each class, we can follow what we did in 4.  In the MapReduce job, we would use 2 reducers and the predicted class value that is outputed from each mapper record becomes partition key.  We then sort the records within each partition using the the newly outputted maximum probability value as the sort key, in reverse order.  Afterwards, we can use unix command head to print the top rows for each class to see the top words with the highest probabilities.  \n",
    "\n",
    "> __f)__ Many of the words with the top probabilities are in the category of natural language fillers I alluded to in (d). Increasing the smoothing parameter k will increase the variance and decrease the bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test/Train split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/enron_train.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/enron_test.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# part a - test/train split (RUN THIS CELL AS IS)\n",
    "!head -n 80 data/enronemail_1h.txt > data/enron_train.txt\n",
    "!tail -n 20 data/enronemail_1h.txt > data/enron_test.txt\n",
    "!hdfs dfs -copyFromLocal data/enron_train.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal data/enron_test.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir NaiveBayes/Unsmoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _without smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8887388945230211309.jar tmpDir=null\n",
      "19/06/02 05:46:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 05:46:08 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 05:46:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/02 05:46:09 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/02 05:46:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0031\n",
      "19/06/02 05:46:10 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0031\n",
      "19/06/02 05:46:10 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0031/\n",
      "19/06/02 05:46:10 INFO mapreduce.Job: Running job: job_1559430286868_0031\n",
      "19/06/02 05:46:21 INFO mapreduce.Job: Job job_1559430286868_0031 running in uber mode : false\n",
      "19/06/02 05:46:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 05:46:33 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/06/02 05:46:34 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 05:47:00 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "19/06/02 05:47:03 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "19/06/02 05:47:05 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "19/06/02 05:47:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 05:47:08 INFO mapreduce.Job: Job job_1559430286868_0031 completed successfully\n",
      "19/06/02 05:47:08 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369553\n",
      "\t\tFILE: Number of bytes written=1642708\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=205255\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19272\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=108876\n",
      "\t\tTotal time spent by all map tasks (ms)=19272\n",
      "\t\tTotal time spent by all reduce tasks (ms)=108876\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19272\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=108876\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=19734528\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=111489024\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=25107\n",
      "\t\tMap output bytes=319315\n",
      "\t\tMap output materialized bytes=369577\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10006\n",
      "\t\tReduce shuffle bytes=369577\n",
      "\t\tReduce input records=25107\n",
      "\t\tReduce output records=4559\n",
      "\t\tSpilled Records=50214\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=637\n",
      "\t\tCPU time spent (ms)=19970\n",
      "\t\tPhysical memory (bytes) snapshot=1435275264\n",
      "\t\tVirtual memory (bytes) snapshot=8252620800\n",
      "\t\tTotal committed heap usage (bytes)=965214208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=205255\n",
      "19/06/02 05:47:08 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-model\n"
     ]
    }
   ],
   "source": [
    "# part b -  Unsmoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\"  \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/enron-model \\\n",
    "  -numReduceTasks 4 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-model/part-000* > NaiveBayes/Unsmoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2.0,4.0,0.0001725476662928134,0.00029682398337785694\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000172547666293,0.000296823983378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1.0,22.0,8.62738331464067e-05,0.001632531908578213\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,8.62738331464e-05,0.00163253190858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _with Laplace +1 smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir NaiveBayes/Smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob3638238434155065464.jar tmpDir=null\n",
      "19/06/02 06:00:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:00:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:00:08 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "19/06/02 06:00:08 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/06/02 06:00:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0036\n",
      "19/06/02 06:00:09 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0036\n",
      "19/06/02 06:00:09 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0036/\n",
      "19/06/02 06:00:09 INFO mapreduce.Job: Running job: job_1559430286868_0036\n",
      "19/06/02 06:00:24 INFO mapreduce.Job: Job job_1559430286868_0036 running in uber mode : false\n",
      "19/06/02 06:00:24 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 06:00:53 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/06/02 06:00:55 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/06/02 06:00:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 06:01:33 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "19/06/02 06:01:38 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "19/06/02 06:01:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 06:01:40 INFO mapreduce.Job: Job job_1559430286868_0036 completed successfully\n",
      "19/06/02 06:01:40 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=228642\n",
      "\t\tFILE: Number of bytes written=1662832\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=205735\n",
      "\t\tHDFS: Number of bytes written=272504\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=107273\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=165833\n",
      "\t\tTotal time spent by all map tasks (ms)=107273\n",
      "\t\tTotal time spent by all reduce tasks (ms)=165833\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=107273\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=165833\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=109847552\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=169812992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4559\n",
      "\t\tMap output records=4575\n",
      "\t\tMap output bytes=219468\n",
      "\t\tMap output materialized bytes=228714\n",
      "\t\tInput split bytes=480\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4572\n",
      "\t\tReduce shuffle bytes=228714\n",
      "\t\tReduce input records=4575\n",
      "\t\tReduce output records=4559\n",
      "\t\tSpilled Records=9150\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=1616\n",
      "\t\tCPU time spent (ms)=23560\n",
      "\t\tPhysical memory (bytes) snapshot=1942315008\n",
      "\t\tVirtual memory (bytes) snapshot=10977849344\n",
      "\t\tTotal committed heap usage (bytes)=1441267712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=205255\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=272504\n",
      "19/06/02 06:01:40 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model\n"
     ]
    }
   ],
   "source": [
    "# part b -  Smoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D stream.map.output.field.separator=\"\\t\" \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\"  \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1\" \\\n",
    "  -files NaiveBayes/train_mapper_smooth.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper_smooth.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/enron-model \\\n",
    "  -output {HDFS_DIR}/smooth-model \\\n",
    "  -numReduceTasks 4 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model/part-000* > NaiveBayes/Smoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2.0,4.0,0.0001858045336306206,0.00027730020520215184\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000185804533631,0.000277300205202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1.0,22.0,0.0001238696890870804,0.0012755809439298986\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,0.000123869689087,0.00127558094393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - write your code in NaiveBayes/evaluation_reducer.py then RUN THIS\n",
    "!chmod a+x NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t1\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t1\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t0\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t0\n",
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t True\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t True\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t True\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t True\n",
      "# Documents 4.0\n",
      "True Positives: 2.0\n",
      "True Negatives: 2.0\n",
      "False Positives: 0.0\n",
      "False Negatives: 0.0\n",
      "Accuracy 1.0\n",
      "Precision 1.0\n",
      "Recall 1.0\n",
      "F-Score 1.0\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test your evaluation job on the chinese model (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py \n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model-results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7161278097575508786.jar tmpDir=null\n",
      "19/06/02 06:03:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:03:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:03:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/02 06:03:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/02 06:03:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0037\n",
      "19/06/02 06:03:17 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0037\n",
      "19/06/02 06:03:17 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0037/\n",
      "19/06/02 06:03:17 INFO mapreduce.Job: Running job: job_1559430286868_0037\n",
      "19/06/02 06:03:31 INFO mapreduce.Job: Job job_1559430286868_0037 running in uber mode : false\n",
      "19/06/02 06:03:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 06:03:49 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 06:04:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 06:04:00 INFO mapreduce.Job: Job job_1559430286868_0037 completed successfully\n",
      "19/06/02 06:04:00 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=818\n",
      "\t\tFILE: Number of bytes written=451278\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49909\n",
      "\t\tHDFS: Number of bytes written=1053\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=30365\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7934\n",
      "\t\tTotal time spent by all map tasks (ms)=30365\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7934\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=30365\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7934\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=31093760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8124416\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=772\n",
      "\t\tMap output materialized bytes=824\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=824\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=210\n",
      "\t\tCPU time spent (ms)=4670\n",
      "\t\tPhysical memory (bytes) snapshot=796327936\n",
      "\t\tVirtual memory (bytes) snapshot=4104740864\n",
      "\t\tTotal committed heap usage (bytes)=627048448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1053\n",
      "19/06/02 06:04:00 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-model-results\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the UNSMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model-results\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/Unsmoothed/NBmodel.txt,NaiveBayes/evaluation_reducer.py \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/enron-model-results \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-model-results/part-000* > NaiveBayes/Unsmoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model-results\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob5262634055469441777.jar tmpDir=null\n",
      "19/06/02 06:06:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:06:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:06:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/06/02 06:06:52 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/06/02 06:06:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0038\n",
      "19/06/02 06:06:53 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0038\n",
      "19/06/02 06:06:53 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0038/\n",
      "19/06/02 06:06:53 INFO mapreduce.Job: Running job: job_1559430286868_0038\n",
      "19/06/02 06:07:08 INFO mapreduce.Job: Job job_1559430286868_0038 running in uber mode : false\n",
      "19/06/02 06:07:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 06:07:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 06:07:35 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 06:07:35 INFO mapreduce.Job: Job job_1559430286868_0038 completed successfully\n",
      "19/06/02 06:07:35 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1349\n",
      "\t\tFILE: Number of bytes written=452337\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49909\n",
      "\t\tHDFS: Number of bytes written=1562\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=26768\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8803\n",
      "\t\tTotal time spent by all map tasks (ms)=26768\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8803\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=26768\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8803\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=27410432\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9014272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=1303\n",
      "\t\tMap output materialized bytes=1355\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1355\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=244\n",
      "\t\tCPU time spent (ms)=4690\n",
      "\t\tPhysical memory (bytes) snapshot=779124736\n",
      "\t\tVirtual memory (bytes) snapshot=4134080512\n",
      "\t\tTotal committed heap usage (bytes)=622854144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1562\n",
      "19/06/02 06:07:35 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model-results\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the SMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model-results\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/Smoothed/NBmodel.txt,NaiveBayes/evaluation_reducer.py \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/smooth-model-results \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model-results/part-000* > NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== UNSMOOTHED MODEL ============\n",
      "# Documents 20.0\t\n",
      "True Positives: 1.0\t\n",
      "True Negatives: 9.0\t\n",
      "False Positives: 0.0\t\n",
      "False Negatives: 10.0\t\n",
      "Accuracy 0.5\t\n",
      "Precision 1.0\t\n",
      "Recall 0.09090909090909091\t\n",
      "F-Score 0.16666666666666669\t\n",
      "=========== SMOOTHED MODEL ============\n",
      "# Documents 20.0\t\n",
      "True Positives: 11.0\t\n",
      "True Negatives: 6.0\t\n",
      "False Positives: 3.0\t\n",
      "False Negatives: 0.0\t\n",
      "Accuracy 0.85\t\n",
      "Precision 0.7857142857142857\t\n",
      "Recall 1.0\t\n",
      "F-Score 0.88\t\n"
     ]
    }
   ],
   "source": [
    "# part c - display results \n",
    "# NOTE: feel free to modify the tail commands to match the format of your results file\n",
    "print('=========== UNSMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Unsmoothed/results.txt\n",
    "print('=========== SMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`EXPECTED RESULTS:`__ \n",
    "<table>\n",
    "<th>Unsmoothed Model</th>\n",
    "<th>Smoothed Model</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t1\n",
    "True Negatives:\t9\n",
    "False Positives:\t0\n",
    "False Negatives:\t10\n",
    "Accuracy\t0.5\n",
    "Precision\t1.0\n",
    "Recall\t0.0909\n",
    "F-Score\t0.1666\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t11\n",
    "True Negatives:\t6\n",
    "False Positives:\t3\n",
    "False Negatives:\t0\n",
    "Accuracy\t0.85\n",
    "Precision\t0.7857\n",
    "Recall\t1.0\n",
    "F-Score\t0.88\n",
    "</pre></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "__`NOTE:`__ _Don't be too disappointed if these seem low to you. We've trained and tested on a very very small corpus... bigger datasets coming soon!_\n",
    "\n",
    "__`part e starts here:`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/smooth-sort': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7269015326975516087.jar tmpDir=null\n",
      "19/06/02 06:14:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:14:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/06/02 06:14:32 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "19/06/02 06:14:32 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "19/06/02 06:14:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1559430286868_0039\n",
      "19/06/02 06:14:33 INFO impl.YarnClientImpl: Submitted application application_1559430286868_0039\n",
      "19/06/02 06:14:33 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1559430286868_0039/\n",
      "19/06/02 06:14:33 INFO mapreduce.Job: Running job: job_1559430286868_0039\n",
      "19/06/02 06:14:43 INFO mapreduce.Job: Job job_1559430286868_0039 running in uber mode : false\n",
      "19/06/02 06:14:43 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/06/02 06:15:17 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "19/06/02 06:15:20 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "19/06/02 06:15:22 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "19/06/02 06:15:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/06/02 06:15:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/06/02 06:15:41 INFO mapreduce.Job: Job job_1559430286868_0039 completed successfully\n",
      "19/06/02 06:15:41 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=409434\n",
      "\t\tFILE: Number of bytes written=1719598\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=272988\n",
      "\t\tHDFS: Number of bytes written=400304\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=135219\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=38716\n",
      "\t\tTotal time spent by all map tasks (ms)=135219\n",
      "\t\tTotal time spent by all reduce tasks (ms)=38716\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=135219\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=38716\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=138464256\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=39645184\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4559\n",
      "\t\tMap output records=4559\n",
      "\t\tMap output bytes=400304\n",
      "\t\tMap output materialized bytes=409470\n",
      "\t\tInput split bytes=484\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4556\n",
      "\t\tReduce shuffle bytes=409470\n",
      "\t\tReduce input records=4559\n",
      "\t\tReduce output records=4559\n",
      "\t\tSpilled Records=9118\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=596\n",
      "\t\tCPU time spent (ms)=27270\n",
      "\t\tPhysical memory (bytes) snapshot=1567203328\n",
      "\t\tVirtual memory (bytes) snapshot=8253276160\n",
      "\t\tTotal committed heap usage (bytes)=1245708288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=272504\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=400304\n",
      "19/06/02 06:15:41 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-sort\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadoop job here (sort smoothed model on P(word|class))\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-sort\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=4 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k4,4nr -k1,1\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k3,3\"  \\\n",
    "  -files NaiveBayes/model_sort_mapper.py \\\n",
    "  -mapper model_sort_mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "  -input {HDFS_DIR}/smooth-model \\\n",
    "  -output {HDFS_DIR}/smooth-sort \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "ClassPriors\t47.0,33.0,0.5875,0.4125\tham\t0.5875\t\n",
      "ClassPriors\t47.0,33.0,0.5875,0.4125\tham\t0.5875\t\n",
      "ClassPriors\t47.0,33.0,0.5875,0.4125\tham\t0.5875\t\n",
      "ClassPriors\t47.0,33.0,0.5875,0.4125\tham\t0.5875\t\n",
      "ect\t378.0,0.0,0.023473306082001735,5.546004104043037e-05\tham\t0.023473306082001735\t\n",
      "and\t258.0,277.0,0.01604112473677691,0.015417891409239643\tham\t0.01604112473677691\t\n",
      "hou\t203.0,0.0,0.0126347082868822,5.546004104043037e-05\tham\t0.0126347082868822\t\n",
      "in\t160.0,157.0,0.009971509971509971,0.008762686484387999\tham\t0.009971509971509971\t\n",
      "for\t148.0,153.0,0.00922829183698749,0.008540846320226277\tham\t0.00922829183698749\t\n",
      "on\t122.0,95.0,0.007617985878855444,0.005324163939881316\tham\t0.007617985878855444\t\n",
      "enron\t116.0,0.0,0.007246376811594203,5.546004104043037e-05\tham\t0.007246376811594203\t\n",
      "i\t113.0,106.0,0.007060572277963582,0.00593422439132605\tham\t0.007060572277963582\t\n",
      "will\t113.0,69.0,0.007060572277963582,0.003882202872830126\tham\t0.007060572277963582\t\n",
      "this\t99.0,90.0,0.00619348445435402,0.005046863734679163\tham\t0.00619348445435402\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t453.0,535.0,0.02811841942276725,0.029726581997670677\tspam\t0.029726581997670677\t\n",
      "to\t350.0,420.0,0.021739130434782608,0.023348677278021184\tspam\t0.023348677278021184\t\n",
      "a\t168.0,274.0,0.010466988727858293,0.015251511286118352\tspam\t0.015251511286118352\t\n",
      "your\t35.0,271.0,0.002229654403567447,0.01508513116299706\tspam\t0.01508513116299706\t\n",
      "of\t188.0,252.0,0.011705685618729096,0.014031390383228884\tspam\t0.014031390383228884\t\n",
      "you\t80.0,252.0,0.005016722408026756,0.014031390383228884\tspam\t0.014031390383228884\t\n",
      "it\t30.0,119.0,0.0019199801808497462,0.0066552049248516446\tspam\t0.0066552049248516446\t\n",
      "com\t74.0,108.0,0.004645113340765515,0.006045144473406911\tspam\t0.006045144473406911\t\n",
      "that\t71.0,100.0,0.004459308807134894,0.005601464145083467\tspam\t0.005601464145083467\t\n",
      "or\t41.0,88.0,0.002601263470828688,0.004935943652598303\tspam\t0.004935943652598303\t\n",
      "are\t55.0,84.0,0.003468351294438251,0.004714103488436582\tspam\t0.004714103488436582\t\n",
      "have\t61.0,75.0,0.0038399603616994923,0.004214963119072708\tspam\t0.004214963119072708\t\n",
      "at\t57.0,66.0,0.003592220983525331,0.0037158227497088346\tspam\t0.0037158227497088346\t\n",
      "s\t50.0,66.0,0.00315867707172055,0.0037158227497088346\tspam\t0.0037158227497088346\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part e - print top words in each class\n",
    "\n",
    "#I print more than 10 lines because of the extra ClassPriors\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/smooth-sort/part-0000{idx} | head -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 ends here, please refer to the `README.md` for submission instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
